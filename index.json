[{"content":"Fugitive is an amazing plugin for Vim. It integrates a lot of Git functionality into the editor. Part of it is that you can just run the commands from inside git like :G commit. But it offers so much more and I want to document how to use some of those features. Mostly for my own sake because I can never remember them when I need them, but I hope it can help others as well.\n:Git This shows the git status list of files. If you want a vertical view you can use :vertical G. A file can be staged or unstaged with s and u.\nThe files can be opened to view the full diff. = toggles the view. \u0026gt; (open) and \u0026lt; (close) can also be used to only open/close it instead of toggle. In the diff view you can stage individual hunks with s and unstage with u. o will open the file at the location under the cursor.\nX can be used to discard a change (either a whole file or the hunk under the cursor).\nDiffsplit Pressing dd (horizontal) or dv (vertical) opens a diff split where you can see the modifications and alter them. By going into visual mode and running :diffput you can stage individual lines rather than hunks. This also makes it easier if you want to save part of a line for the next commit. Copy the line, change one version to the \u0026ldquo;first commit\u0026rdquo; version, and keep the other line for the second version. Then use diffput to stage only the first version. Then delete the version you just staged. Now you have staged part of a line. This can be useful for example when you have implemented some functionality but deem it should be two separate commits, one that introduces the base functionality and another that adds some more complex arguments.\nNote that you should only modify the right/bottom file of the split, as that is the one that reflects your current state. The other reflects how it looked before your changes.\n","permalink":"https://casan.se/blog/misc/vim-fugitive-notes-for-myself/","summary":"Fugitive is an amazing plugin for Vim. It integrates a lot of Git functionality into the editor. Part of it is that you can just run the commands from inside git like :G commit. But it offers so much more and I want to document how to use some of those features. Mostly for my own sake because I can never remember them when I need them, but I hope it can help others as well.","title":"Vim Fugitive notes for myself"},{"content":"The Precision Time Protocol, PTP, is used for synchronizing time across the network. But what if the clocks I want to synchronize are all internal?\nIn previous posts, I have explained parts of PTP and how that is used to synchronize devices in a network. PTP is often done with the ptp4l daemon from the Linuxptp project, and it is overall a well-explained protocol across the web. There are many resources. The project also provides a couple of other tools, one of which is ts2phc, which has way less documentation about it online. This post will focus on what it does and why it\u0026rsquo;s needed.\nWhy is it needed? Some network switches will only have a single PHC that is shared across all ports. In this case, no synchronization is needed. If instead the PHC and timestamping are located in the PHY there is one PHC per port (or 2/4 ports for dual/quad PHY). When running PTP across multiple PHYs their time needs to be synchronized. For a Transparent Clock, they will never be adjusted by PTP, but they must still have the same time. Running ts2phc on them will ensure that they always stay within a few nanoseconds of each other. For a Boundary Clock, the time of one PHY will constantly be adjusted by PTP (the port towards the PTP Grandmaster), while the other ports act as masters for downstream clocks. Internally the PHY towards the Grandmaster will act as a master for the other PHY clocks to make sure that the time transmitted downstream is in sync with upstream.\nFor Transparent Clocks, a single alignment pulse can sometimes be used to make all clocks start their time counting at the same time, and then never need to be synchronized again. But this depends on the PHYs and there is no official support for this in Linux and requires all PHYs to run on the same oscillator.\nHow synchronization works Internal clocks do not have any network connections between them, so reusing the PTP daemon isn\u0026rsquo;t an option. The naive approach would be to read out the current time from each clock, compare them, and adjust the clocks accordingly. This is what the Linuxptp application phc2sys does. But as the name suggests the purpose of it is to synchronize the time of a Physical Hardware Clock (PHC) to the system. Or vice versa if the system acts as the Grandmaster clock of the network. As for synchronizing two PHCs this simply isn\u0026rsquo;t good enough. The precision will be unacceptable compared to the precision you can reach with PTP; a Boundary/Transparent clock that does this will probably introduce more inaccuracy than if it just forwarded the PTP packet in hardware.\nTo the rescue comes the Linuxptp application ts2phc. This application needs some specialized hardware design to work. The issue with phc2sys is that comparing timestamps that are taken at different times inherently gives inaccurate results. For perfect synchronization, the timestamps must be taken as close together as possible. The solution for this is a signal line in the PCB. Each PHC is set up by ts2phc to take a timestamp when a pulse is received on that line. A pulse is then triggered and timestamps are read out from all PHC, compared, and adjusted.\nOn the hardware, the different PHCs need to be connected on a single line, as shown below. One of the clocks will be configured to send out the pulse, while the other two are configured to timestamp the pulse. The line may be subject to some propagation delay on large boards. Online resources suggest 1 nanosecond per 15cm1. Which for normal PTP operation probably isn\u0026rsquo;t enough to worry about.\n+-------+-------+ | | | +----+ +----+ +----+ |PHC1| |PHC2| |PHC3| +----+ +----+ +----+ Linux kernel API The Linux kernel exposes two APIs for this purpose. EXTTS (EXTernal TimeStamp) configures the PHC to timestamp on a signal on a specific hardware pin. PEROUT (PERiodic OUTput) configures a PPS (Pulse-Per-Second) signal on one of the PHCs that will trigger every time the PHC is at a whole second time.\nThe PHC dedicated as master will be configured with PEROUT, while the rest are EXTTS. Because the PPS is always triggered at a whole second it can find the timestamp of the master PHC by reading the current time and rounding down, assuming we do it within a second (i.e. before the next signal). In theory, however, there is no requirement for the pulse to happen every second. It\u0026rsquo;s fully possible to have something external trigger a pulse to all PHC, including the master (the master also needs to be configured for EXTTS in this case). It\u0026rsquo;s also possible for the PHC sending PPS to be treated as slave and be adjusted. What is important is that there is a timestamp from every clock that we can compare.\nCombining it with PTP For every setup there needs to be one master PHC and the rest are slaves. The slaves are compared to and adjusted to match the time of the master. ts2phc expects one PHC to be configured as master. But there are cases when a dynamic setup is required, e.g. when running a PTP Boundary Clock across the PHCs. The master PHC must always be the one acting as PTP slave, the one in the direction of the Grandmaster, as this is the one being adjusted from an external source. This time must be distributed to all other PHCs for the time to continue being accurate downstream. If the Grandmaster changes position and another PHC is now being adjusted by PTP then ts2phc also requires changing master. To do this the command line argument ts2phc -a can be used to listen to port state events from ptp4l. Now ts2phc will always match its master to the PTP slave port.\nIt may also be useful to set the option ts2phc --step_threshold=0.0001 to allow stepping the clocks. Otherwise, the clocks can end up stabilizing internally before a Grandmaster is found, and will then take a long time to converge.\nhttps://madpcb.com/glossary/propagation-delay\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://casan.se/blog/networks/ts2phc-and-synchronizing-hardware-clocks/","summary":"The Precision Time Protocol, PTP, is used for synchronizing time across the network. But what if the clocks I want to synchronize are all internal?\nIn previous posts, I have explained parts of PTP and how that is used to synchronize devices in a network. PTP is often done with the ptp4l daemon from the Linuxptp project, and it is overall a well-explained protocol across the web. There are many resources.","title":"ts2phc and synchronizing hardware clocks"},{"content":"To accurately synchronize the time we need to account for all time lost as the packet travels. One part of this is the time it takes for packets to travel across the cable, including egress time after the timestamp is taken, and ingress time before the timestamp is taken. Though measuring the time it takes from one device to another is impossible because they will never have the exact same time, so a timestamp on one side is not guaranteed to match up with a timestamp on the other side. We have to settle for measuring the roundtrip time and dividing it by 2, and hope that the delay is symmetric.\nTwo-step peer delay This is similar to how two-step for synchronization works, we send a packet, timestamp the egress time, and send a follow-up packet containing the egress time of the first. Two-step peer delay can be done in two different ways, but let us explore the concept first.\nTwo-step using full timestamps Two devices running PTP in peer delay mode will both measure the peer delay independently. But here I will focus on the transaction from the perspective of one side. Call one side client and the other side the server. In reality, both will be both client and server.\nClient sends a peer delay request PDELAY_REQ. It is timestamped t1 on egress and kept by the client. Server receives PDELAY_REQ and timestamps it t2. Server sends a peer delay response PDELAY_RESP containing t2. It is timestamped t3. Client receives PDELAY_RESP containing t2. It is timestamped t4. Server sends a peer delay response follow-up PDELAY_RESP_FUP containing t3. Client receives PDELAY_RESP_FUP containing t3. Now the client has all 4 timestamps:\nt1: egress time of request t2: ingress time of request t3: egress time of response t4: ingress time of response ________t1 t2________ |Client|\u0026gt;-----\u0026gt;|Server| |______|\u0026lt;-----\u0026lt;|______| t4 t3 The formula for calculating the peer delay (meanPathDelay) time looks like this\nmeanPathDelay = ((t4-t1) - (t3-t2))/2 t3-t2 represents the time it spent inside the server device, often called residence time. t4-t1 represents the total time the packet was away. The total time on the wire is then found by subtracting the residence time from the total time. Dividing by 2 gives the mean delay.\nTwo-step using correctionField In this method all the timestamps are still taken the same way as above. But instead of sending the timestamps back, those fields are simply set to zero, and the time t3-t2 is added to correctionField of the PDELAY_RESP_FUP.\nThe full formula is actually slightly longer than what was shown above, to allow for other forms of compensation\nmeanPathDelay = ((t4-t1) - (t3-t2) - pdelay_resp.correction - pdelay_resp_fup.correction)/2 Here we will now have t2 and t3 set to zero on the clients side. And that time instead comes in the pdelay_resp_fup.correction. The end result is the same. And the server side can choose which method to use since it doesn\u0026rsquo;t matter to the client.\nCorrectionField contains a number of nanoseconds that should be subtracted by the receiver. This exists to allow other forms of compensation as well. The Switchcore or PHY may add compensation for the time it introduces after timestamping.\nOne-step peer delay This works similar to one-step synchronization, and uses the correctionField approach explained above. For this the response ingress timestamp t2 is placed in the reserved2 field (4 bytes) before the response is sent. This field is smaller than the full timestamp, so it only has 2^32 time units to send the response. Past that it will no longer match to the correct ingress time. If the time units are nanoseconds then it calculates to ~4.3 seconds, which is plenty of time to send a response.\nOn egress, the timestamp t3 is taken, and t3-t2 is added to the correctionField on the fly. Now the residence time is calculated and sent back to the client, which now uses the following formula\nmeanPathDelay = ((t4-t1) - pdelay_resp.correction)/2 Note that the full formula from above can still be used, as the unused values will be 0 (and there will be no follow-up).\n1.5-step peer delay This is a combination of two-step and one-step, and can be applied to both sync and peer delay1. The idea is that the protocol still behaves the same as two-step. I.e. it sends follow-up packets. The difference here is that the timestamp, e.g. t1, is just kept in the timestamping hardware and is not read out by the CPU. When the matching follow-up (requesting port ID and sequence ID matches) comes to the hardware the timestamp is filled in. When it goes on the wire it now looks like a normal two-step.\nThe benefit of this is because one-step adds latency to the transmission as it has to stop, take the timestamp just before transmitting, and then modify the packet. At speeds of 10Gbit and higher it starts affecting the transmission speed. The follow-up doesn\u0026rsquo;t have to be modified exactly at transmission time, it can be prepared ahead of time so it doesn\u0026rsquo;t affect the latency. By eliminating the readout it minimizes the overhead of sending packets. Meaning the follow-up packet can be sent immediately after the initial sync/pdelay_resp.\n1.5-step is not formulated in any standard because it does not affect the perception. From a protocol point-of-view it is still two-step.\nhttps://www.ieee802.org/1/files/public/docs2015/ASRev-pannell-To-1-step-or-not-0315-v1.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://casan.se/blog/networks/ptp-peer-delay-measurement/","summary":"To accurately synchronize the time we need to account for all time lost as the packet travels. One part of this is the time it takes for packets to travel across the cable, including egress time after the timestamp is taken, and ingress time before the timestamp is taken. Though measuring the time it takes from one device to another is impossible because they will never have the exact same time, so a timestamp on one side is not guaranteed to match up with a timestamp on the other side.","title":"PTP peer delay measurement"},{"content":" Disclaimer: I am not a lawyer. Please consult with a lawyer if you have any doubts regarding licensing and copyright.\nWith the advent of AI code generators, the question of copyright and ownership has arisen. AI generates new code based on what it was trained on, but there have been cases of it outputting exact matches of code available online. This poses a problem. How will a human user ever know whether the output is legally allowed to be used? It has been shown that ChatGPT (not sure which version) was able to output code licensed under GPL, which could be devastating if included in a proprietary codebase. The responsibility should lie on the AI developers to only use data that will not cause any legal issues. If they are offering a service that could potentially output copyrighted/licensed code it should have a clear disclaimer and could never be used for business or open-source purposes. However, it is still fine for personal use.\nPermissively licensed code can technically be used freely, but there is still the issue of preserving the copyright and license for that code. An AI will likely lose that information when it \u0026ldquo;generates\u0026rdquo; the code. The code available for training a \u0026ldquo;business-grade AI\u0026rdquo; would have to be anything in the public domain (for those who want to license code to the public domain there is the CC0 license).\nEven when the code is generated by the AI (i.e. not a direct copy of code found elsewhere), who can claim copyright? David Gewirtz did some digging into this and the copyright likely falls into the public domain for the same reason that any work produced by nature, animals, or plants does. An AI is not a legal entity and can therefore not hold any copyright. Though there have yet to be any big lawsuits around AI-generated code.\nThere was one case regarding a graphic novel created with the help of the AI Midjourney. The conclusion from The U.S. Copyright Office was that, while the work itself was copyrightable because it required human work to add text and put it all together, the images generated by Midjourney were not subject to copyright.\nHow the licensing issues play out we will probably see in the coming years. I\u0026rsquo;m very wary of using any AI-generated code and will likely avoid it in the foreseeable future.\n","permalink":"https://casan.se/blog/programming/my-thoughts-on-ai-code-generators/","summary":"Disclaimer: I am not a lawyer. Please consult with a lawyer if you have any doubts regarding licensing and copyright.\nWith the advent of AI code generators, the question of copyright and ownership has arisen. AI generates new code based on what it was trained on, but there have been cases of it outputting exact matches of code available online. This poses a problem. How will a human user ever know whether the output is legally allowed to be used?","title":"My thoughts on AI code generators"},{"content":" Disclaimer: I am not a lawyer. Please consult with a lawyer if you have any doubts regarding licensing and copyright.\nLicensing Why do we license open-source code? Why is it not enough to just make it publicly available? Because all code that is not explicitly licensed falls under the default copyright laws, meaning that the authors of the code own everything and it may not be used in derivative work. If the authors work for a company it is typically the company that owns the work since it was done on their dime, and if the code is proprietary then there is no need to license it. Licensing is strictly required when you want others to be able to use the code.\nIn GitHub\u0026rsquo;s guide to licensing, they make it clear that this is the case:\nYou\u0026rsquo;re under no obligation to choose a license. However, without a license, the default copyright laws apply, meaning that you retain all rights to your source code and no one may reproduce, distribute, or create derivative works from your work. If you\u0026rsquo;re creating an open-source project, we strongly encourage you to include an open-source license.\n\u0026ndash; GitHub - licensing a repository\nWhen starting a new project you do not have to choose a license immediately. It can be added down the line when you have decided the goal of your software, but until then you as the author retain full copyright. A license can also be changed down the line; the only caveat is that you need the permission of all copyright holders in the project. If many people have already contributed to a project it would take all of them agreeing on the license change. Keep in mind that changing the license will only affect the code as it is when you add the new license. Any previously released iterations of the code will remain under the license with which it was released. This could be the Git history (unless you choose to completely rewrite the history) or anyone who downloaded it.\nA license should be picked according to how you wish your project to be distributed. Two popular open-source licenses are MIT and GPLv2. MIT allows anyone to do anything with the code as long as the copyright and license notices are preserved. GPLv2, which is famous for being the license used by the Linux kernel, requires any modifications or derivative works to be made available publicly. This prevents anyone from making proprietary changes.\nIf you are struggling to choose a license take a look at choosealicense.com.\nFor a more rigorous explanation, opensource.guide/legal has some good but still quite easily digestible content.\nREUSE Software Code licensing is tricky, and it gets even trickier when you want to copy code from other places to use in your code. What license some code uses may not be immediately obvious. GitHub does display the license of a repository, but only if it is placed in a way where GitHub is set to look for it. Far from all projects follow their conventions. And even then, it only displays one license. Projects can be licensed under multiple licenses depending on file type or where the code was taken from. Even a single file can have sections of code that fall under a different license. Dual licensing is another possibility that allows you to choose which of the listed licenses to redistribute the code with.\nREUSE was created by the Free Software Foundation Europe (FSFE) and aims to simplify handling code licensing by providing a recommendation for how licensing should be done, as well as a tool to apply and check licenses. It uses the SPDX standardized methods for license and copyright of code. When a project is compliant with REUSE anyone can easily check the copyrights of a file.\nREUSE requires licensing all files to be compliant, except for any files listed in .gitignore since they will never be included in the distribution of the source code. The default method of licensing is to have a header in each file that looks like this, using the comment style of the language corresponding to the file extension.\n# SPDX-FileCopyrightText: 2019 Jane Doe \u0026lt;jane@example.com\u0026gt; # # SPDX-License-Identifier: GPL-3.0-or-later The year and contact address may be omitted, but the name of the copyright holder must always be included. If there are multiple copyright holders they should all be listed individually. The license is indicated by an identifier from the SPDX Specification.\nFor files that can\u0026rsquo;t contain a license header (configuration files, images, etc.) there are other ways. The documentation lists three methods for licensing a file.\nPlacing tags in the header of the file. Placing tags in a .license file adjacent to the file. Putting the information in the DEP5 file. To get started with REUSE you can install their tool and run reuse init in a project. This will ask for a license and create the .reuse/dep5 file. To quickly add a license header to all existing files use the command reuse addheader. To download a copy of all licenses used in the project use reuse download --all (remember to edit it to contain the year and copyright holder names, some licenses will have a placeholder where you should write it). To check if your project is compliant use reuse lint. It is also a good idea to add this to your CI pipeline, to avoid a badly licensed version of the repository being distributed. Check out their website for full details on how to install and use it.\nI believe that REUSE has a bright future and I encourage you to use it in your projects too. Let\u0026rsquo;s improve the world of open source together, one project at a time.\n","permalink":"https://casan.se/blog/programming/licensing-and-the-reuse-software-initiative/","summary":"Disclaimer: I am not a lawyer. Please consult with a lawyer if you have any doubts regarding licensing and copyright.\nLicensing Why do we license open-source code? Why is it not enough to just make it publicly available? Because all code that is not explicitly licensed falls under the default copyright laws, meaning that the authors of the code own everything and it may not be used in derivative work.","title":"Licensing and the REUSE Software initiative"},{"content":"I thought it would be fun to look at all the different commands I use. I took a peek into my Zsh history and extracted just the commands (without any arguments). Below is the command I used. It also removes anything that starts with ., ~, and / to avoid local scripts and binaries. My original intent was to publish the whole list, but it\u0026rsquo;s long, and over half of it is just misspelled commands and a lot of aliases. And frankly, most of it is not very interesting. Instead, here is a curated selection just for you. I tried to include some lesser known commands/uses since other similar lists tend to have a huge overlap. I may revisit this post in the future to add more commands.\nhistory | sed -e \u0026#39;s/^[[:space:]]*//\u0026#39; | cut -d\u0026#39; \u0026#39; -f3 | sed -e \u0026#39;s/^[\\.\\/~].*//g\u0026#39; | sort | uniq capmon I\u0026rsquo;m starting this list with a shameless plug of my command, and it only happened to be first due to alphabetical order ;). Run it with a command to check which Linux kernel capabilities the command requires. E.g. capmon \u0026quot;tcpdump -i eth0\u0026quot;. As someone who loves and uses capabilities, this is very helpful. Check it out here.\nerrno Because I can never remember what the different exit codes mean I use a command to check it. Just errno -l is enough, but I prefer to reorder the output to make it easier to read:\nerrno -l | awk \u0026#39;{ printf(\u0026#34;%s %s %s\\n\u0026#34;, $2, $1, substr($0,2)); }\u0026#39; cloc Count Lines Of Code does exactly what it says. Counts how many lines of code you have in different languages and how much of it is comments or blank lines, and displays it in a pretty table. Check it out here.\nrg Ripgrep is an awesome program for recursively searching directories for some text. It\u0026rsquo;s quite well known, but I had to include it here because it\u0026rsquo;s essential to me. Check it out here.\nfd Like Ripgrep, this is another incredibly useful utility tool for searching, but for files rather than inside files. It is an improved version of the standard Linux command find. Check it out here.\ndelta A modern diff viewer with a pretty output and syntax highlighting. I use this as the pager for git diff. Check it out here.\nnautilus This is the standard file explorer for Ubuntu and probably many more distros. If you would like to open a directory with the file explorer from the terminal (rather than navigating to it via the GUI) you can run this command with the desired path. E.g. nautilus . to open the current directory.\npython Now this may look stupid, but I wanted to list it here because whenever I need a calculator I open a terminal and start the REPL. Even though I rarely write actual Python code.\nrofi A pretty alternative to dmenu for creating interactive and searchable lists and selecting an item. Check it out here.\nreuse Reuse is an initiative by the Free Software Foundation to standardize how code licenses are indicated in projects and source files. Along with it exists the command of the same name. My mostly used is reuse lint to check if my project is compliant.\nrifle This command comes along when you install the ranger command (a TUI file explorer). It opens a file with the correct (as good as it can) program. There already exists a program called xdg-open that does this, and which is included on most distros. But I find rifle to be much better at picking the right application.\ntac tac is the reverse of cat. And that\u0026rsquo;s just what it does. Any input to cat is just spat out the same way it came in. tac reverses the order of the lines. I often use this inside Vim by selecting some lines, then running it from Vim with :!tac. (Note: placing an exclamation mark at the start of a Vim command runs it as a shell command).\n","permalink":"https://casan.se/blog/misc/useful-commands-i-use/","summary":"I thought it would be fun to look at all the different commands I use. I took a peek into my Zsh history and extracted just the commands (without any arguments). Below is the command I used. It also removes anything that starts with ., ~, and / to avoid local scripts and binaries. My original intent was to publish the whole list, but it\u0026rsquo;s long, and over half of it is just misspelled commands and a lot of aliases.","title":"Useful commands I use"},{"content":"I often see git commit -m \u0026quot;My commit\u0026quot; as a beginner example of how to create commits. It is a simple way to quickly create a commit with a short explanation. My issue with it is that it teaches you to write short commit messages. You only fit so much in the terminal before it starts wrapping around and getting hard to work with, and you can\u0026rsquo;t do any proper formatting. Granted, there may be cases where a short text is enough to explain the commit. I, however, don\u0026rsquo;t believe this should be your default approach.\nA good commit explains why something was done and why it was done in that particular way. Looking at the Git history, git log and git blame, is essential in any large codebase. It sucks looking at a commit that introduced some change and all it says is \u0026ldquo;Changed X to Y because Z\u0026rdquo;. Why did Z require this change? Was Z a bug? If so, what caused the bug in X and how did it manifest? What benefits does Y have? New features may need to contain some general information on what the commit as a whole does.\nUsing your editor When not using -m Git defaults to using an editor to open it as a file. The editor to use is defined in your .gitconfig file\n[core] editor = nvim or simply through git config --global core.editor nvim. Replace nvim with whatever editor you wish to use.\nThe editor opens it as a file where you write the commit message. When you are done, save the file and close it. This causes the commit to finish.\nThe benefits of editors Using your editor allows you to carefully think through your commit message before saving and closing it. But the main benefit is that it can be formatted into title/body texts. On top of that, it can also be used to add lists or ASCII art to explain the commit. As an example, I\u0026rsquo;m going to use a commit I recently made to an open source project.\nptp: Parse major and minor version correctly In IEEE1588-2008, the lower 4 bits of the field were defined as the PTP version, and the upper 4 bits were marked as reserved. As of the 2019 version the upper 4 bits are defined as PTP minor version. Previously, the whole byte was assumed to be the version by Tcpdump, resulting in PTP v2.1 being parsed as v18 and not printing the rest of the information. With this commit the version is parsed correctly and the packet is displayed as v2. By leaving an empty line before the second paragraph it creates the separation of title/body. The first line becomes the title, the rest becomes the body. When viewing in short form (e.g. on GitHub commit history) it only shows the title and you have to click the three dots to expand the body. And if the title is too long it gets truncated and the rest ends up in the body anyways. To make it easy to browse keep your title concise at a maximum of 80 characters so the whole title is shown. Most editors show the column number of your cursor somewhere on the screen (usually at the bottom).\nIn the title, I give a short explanation of what the commit does. In the body, I give a longer explanation of why this happened, what the resulting bug was, and why my change is what it is. For general examples of good commit messages, I refer you to the Linux kernel. They have a high standard on commits that are accepted, and rightly so due to the immense size of the codebase and the rate at which it changes.\nAs for how it looks on GitHub, here is both the title-only view and the full commit view.\nAs a final note, I would like to add that, though I always write my commits in my editor, I am nowhere near as rigorous as the above example when it comes to my personal projects. But as a project becomes larger and more serious the Git history needs to be taken more seriously as well. Have the mindset right from the beginning, and you will be able to do it properly when needed.\n","permalink":"https://casan.se/blog/misc/dont-use-git-commit-m/","summary":"I often see git commit -m \u0026quot;My commit\u0026quot; as a beginner example of how to create commits. It is a simple way to quickly create a commit with a short explanation. My issue with it is that it teaches you to write short commit messages. You only fit so much in the terminal before it starts wrapping around and getting hard to work with, and you can\u0026rsquo;t do any proper formatting.","title":"Don't use 'git commit -m'"},{"content":"Setting two clocks to the same time doesn\u0026rsquo;t seem that hard, right? If same-second precision is enough for you then you can usually do that. But when we are talking nanosecond precision like that provided by PTP (IEEE1588, Precision Time Protocol) there are some more variables to consider.\nSynchronization The first variable to note is the time it takes to set the new time. Even if the action to set the time is triggered at the exact same time, the OS will introduce latency in the operation. Anything that depends on the OS being precise in its operations is out of the question. For this reason the adjustments should be done through adjustments rather than setting an absolute time. By knowing the difference between two clocks the time can be adjusted by that much to synchronize them. Doing this with PTP is explained in this post.\nAssuming the initial difference calculated is precise then this will bring the time difference hopefully within maybe a microsecond.\nSyntonization Now that the clocks are nearly aligned we need to consider the frequency of the oscillators driving the clocks. If two clocks are driven by oscillators at different frequencies, or even just of different qualities, it will result in wander. A big difference will allow significant wander in between every synchronization, resulting in never reaching near-perfect accuracy. When the clocks are within a certain threshold time adjustment can stop and clock syntonization starts. The slave clocks will adjust their operating frequency. Though not literally the frequency, but rather a compensation value the clock will take into account when advancing its time. The frequency is adjusted so the time difference between the clocks approaches 0. If the slave time is behind it needs to increase the frequency gradually until it catches up, at which point it will start fluctuating back and fourth just around the frequency of the master.\nHoldover Clock holdover time is how long it can stay within a certain offset from the master after losing the synchronization (e.g. losing connection). A typical requirement is that it should keep some accuracy during 5 seconds loss. In terms of PTP this may happen if its master clock goes down and it needs to find an new master. And during those 5 seconds while waiting for a new master it cannot drift too far away from the initial time.\nPhysical Hardware Clock When requiring nanosecond-precision timekeeping a hardware clock is used. In Linux this is referred to as a Physical Hardware Clock (PHC). It keeps the time and allows some operations to manage it.\nSet time Get time Adjust time Set frequency Points 1) and 2) are not required by PTP, but are useful for debugging. 3) is the one that is used to adjust for the difference. 4) sets the frequency of the PHC. These operations can be accessed using the phc_ctl tool from the Linuxptp project (along with some more operations).\nIn Linux the PHC are exposed as devices under /dev, typically as /dev/ptpN, where N denotes the number of the PHC.\nSet time The time set is an absolute value in Unix time. Meaning a value of 10 sets the time to 1970-01-01 00:00:10.\nphc_ctl /dev/ptp0 set 10 Get time Reads out the time\nphc_ctl /dev/ptp0 get Adjust time Increments or decrements the clock by the given time in seconds, read as double precision floating point values.\nphc_ctl /dev/ptp0 adj 10.5 Set frequency Sets the frequency of the clock in ppb (parts-per-billion), meaning how many nanoseconds per second it should it should stray from its real frequency. Note that each PHC has different maximum frequency adjustments. The PHC capabilities can be viewed with phc_ctl /dev/ptp0.\nphc_ctl /dev/ptp0 freq 1222 Time scales and leap seconds International Atomic Time (TAI) operates based on a weighted average of over 450 atomic clocks spread out around earth. Universal Coordinated Time (UTC) is based on the actual earths rotation. In TAI a day is defined as having exactly 86 400 seconds, while UTC is subject to leap seconds. Leap seconds occur due to earths rotation speed always changing slightly due to geological or climatic changes, e.g. the continental plates moving or the ice caps melting. Since this cannot be predicted over longer periods a leap second is only announced 6 months in advance.\nThe leap seconds always happen on either June 30th or December 31st at 23:59:59. A leap second ahead is shown as 23:59:60, before advancing to 00:00:00. A leap second backwards goes directly from 23:59:58 to 00:00:00. As for when a leap second occurs, a Linux system should be informed by an NTP daemon when one is scheduled, which in turn gets its time through NTP from a more authoritative clock device.\nPTP operates on the TAI timescale, but always informs of the current UTC offset (37 at the time of writing) to make sure any devices that need the time in UTC can convert to it.\n","permalink":"https://casan.se/blog/networks/synchronization-and-syntonization/","summary":"Setting two clocks to the same time doesn\u0026rsquo;t seem that hard, right? If same-second precision is enough for you then you can usually do that. But when we are talking nanosecond precision like that provided by PTP (IEEE1588, Precision Time Protocol) there are some more variables to consider.\nSynchronization The first variable to note is the time it takes to set the new time. Even if the action to set the time is triggered at the exact same time, the OS will introduce latency in the operation.","title":"Synchronization and Syntonization"},{"content":" Measure the time a packet is on the wire accurately using hardware timestamping. This is useful for measuring the impact of traffic congestion and testing QoS features. The timestamped packets are intended to never touch any software on their trip through the network since that adds considerable delay and jitter, and the benefit of hardware timestamping dwindles. In which case using ping may be enough precision.\nIf you are unfamiliar with timestamping I recommend checking out my post on PTP and timestamping methods.\nGitHub repository\nUsing Wiretime Wiretime requires two interfaces, one for transmission and one for receival. This can be the same physical port by creating two VLAN interfaces on top of the port interface (note that they must be attached directly to the port and not through a bridge as bridges cannot do timestamping). Ideally, Wiretime should be used on a network switch that has multiple interface capable of timestamping. For best accuracy the ports should use the same Physical Hardware Clock (PHC). If they aren\u0026rsquo;t using the same PHC they need to be precisely synced.\nThe packets will use a path that loops back to the transmitting switch. The receiving port should be set to a different VLAN than the transmitting port, or removed from the bridge completely, to avoid flooding. Packets are timestamped on transmission and receival and the difference is calculated across several packets and the average is taken.\nThe most basic command looks like this\nwiretime --tx eth1 --rx eth2 The following is an illustration of an example setup. Wiretime runs on SW1 and transmits on one port and receives on another. The packet is switched in hardware through SW2, and then back to SW1.\n┌───────┐ │ │ ┌───▲─┐ ┌─▼───┐ │ SW1 │ │ SW2 │ └───▲─┘ └─▼───┘ │ │ └───────┘ Flags -t, --tx \u0026lt;interface\u0026gt; Transmit packets on \u0026lt;interface\u0026gt;. Can be a VLAN or other interface, as long as the physical port supports hardware timestamping.\n-r, --rx \u0026lt;interface\u0026gt; Receive packets on interface \u0026lt;interface\u0026gt;. Can be a VLAN or other interface, as long as the physical port supports hardware timestamping.\n-p, --pcp \u0026lt;PCP\u0026gt; PCP priority. If VLAN is not set it will use VLAN 0.\n-v, --vlan \u0026lt;VID\u0026gt; VID to tag the packet with.\n-P, --prio \u0026lt;priority\u0026gt; Socket priority. Used to achieve egress QoS.\n-o, --one-step Use one-step TX instead of two-step.\n-O, --out \u0026lt;filename\u0026gt; Output data into file for plotting. Use when running Wiretime on a device that does not have Gnuplot installed. The file can then be copied to another device for plotting afterwards.\n-i, --interval \u0026lt;milliseconds\u0026gt; Interval between packets. Default: 1000.\n-b, --batch_size \u0026lt;count\u0026gt; Amount of packets to include in every output. Outputs the average time of all packets in the batch. Default: 1.\n-S, --software_tstamp Perform software timestamping instead of hardware timestamping.\n--plot \u0026lt;filename\u0026gt; Plots the data using Gnuplot and exports as PDF. If -O is not used it will create a temporary file for storing the data. The same plotting settings also exists as a bash script in the repository.\n--tstamp-all Enable timestamping of non-PTP packets. On some NICs this will behave differently than timestamping PTP packets only. Incompatible with --one-step.\nExample plot Below is an example plot using one-step PHY timestamping taken over 110 seconds. The packets are sent from one switch, through another, and back to the senders receiving port. The total time spent on the wire is around 3500\u0026ndash;4000 nanoseconds, or 3.5\u0026ndash;4 microseconds.\n","permalink":"https://casan.se/docs/wiretime/","summary":"Measure the time a packet is on the wire accurately using hardware timestamping. This is useful for measuring the impact of traffic congestion and testing QoS features. The timestamped packets are intended to never touch any software on their trip through the network since that adds considerable delay and jitter, and the benefit of hardware timestamping dwindles. In which case using ping may be enough precision.\nIf you are unfamiliar with timestamping I recommend checking out my post on PTP and timestamping methods.","title":"Wiretime"},{"content":"Send hexdumps copied from Tcpdump/Wireshark. Save the hexdump in a file and give it to hexend to send it on an interface. Useful for repeating a specific captured frame, instead of making a pcap playback or trying to recreate it using tools like Nemesis or EasyFrames. It is also possible to write the frame by hand if you so wish.\nGitHub repository\nUsing Hexend The input may only contain hexadecimal characters and whitespace.\nSend a frame from file to eth0. Repeats until stopped.\nhexend eth0 my_frames/frame.hex Send a frame from file, repeat 10 times and suppress output\nhexend eth0 my_frames/frame.hex -c 10 -q Pipe file contents to input, repeat 5 times with 0.1 second interval\ncat my_frames/frame.hex | hexend eth0 -c 5 -i 0.1 Pipe raw string to input, repeat 1000 times with no interval\necho ffffffffffffaaaaaaaaaaaa0000 | hexend eth0 -c 1000 -i 0 Alternative If you just want a simple alternative to this you can use a script like\n#!/bin/sh cat $2 | xxd -r -p | socat -u STDIN interface:$1 that will behave like Hexend regarding the input, but does not support any flags. Looping this is also very inefficient for sending many frames quickly.\n","permalink":"https://casan.se/docs/hexend/","summary":"Send hexdumps copied from Tcpdump/Wireshark. Save the hexdump in a file and give it to hexend to send it on an interface. Useful for repeating a specific captured frame, instead of making a pcap playback or trying to recreate it using tools like Nemesis or EasyFrames. It is also possible to write the frame by hand if you so wish.\nGitHub repository\nUsing Hexend The input may only contain hexadecimal characters and whitespace.","title":"Hexend"},{"content":"Monitor when processes check capabilities(7) to find out what they require. Run capmon '\u0026lt;cmd\u0026gt;' to track the capabilities it requires. It\u0026rsquo;s important to not run it with sudo since that bypasses certain checks. Let the command fail at the first missing capability check and add that capability, then try again and see if if fails on more. Rinse and repeat until command runs successfully.\nIf you are not familiar with capabilities can read A simpler life without sudo.\nCapmon - GitHub\nUsing Capmon It is recommended to use Capmon without sudo, since running it with sudo the provided command inherits the sudo properties and will bypass several checks. Capmon requires CAP_DAC_OVERRIDE and CAP_SYS_ADMIN.\nTo use Capmon do\ncapmon \u0026#39;\u0026lt;cmd\u0026gt;\u0026#39; For example:\ncapmon \u0026#39;ip link netns add test\u0026#39; It is recommended to enclose the command in quotes to avoid the shell from doing any funny business with globbing or other special features, and to avoid Capmon from interpreting the command\u0026rsquo;s argument as its own. Capmon will run the command with /bin/sh.\nThe output of the above command will be\n[ip] - [PASS] CAP_DAC_OVERRIDE - [PASS] CAP_NET_ADMIN because the ip command required the capabilities CAP_NET_ADMIN and CAP_DAC_OVERRIDE for this particular task. Another example, ip link set dev tap0 up only requires CAP_NET_ADMIN.\nIf the user didn\u0026rsquo;t have the capabilities it would instead report [FAIL] on one of the capabilities. If it failed on the first of the two then it may not even show the second since commands often bail out as soon as they fail to do something.\nIf a command is still failing even though Capmon doesn\u0026rsquo;t report anything there is the flag -a or --all. This changes the place where it listens to another location which covers many more checks, some of which are not always necessary and are allowed to fail. This is not the default mode as to not confuse the user with a bunch of capabilities that usually will not matter.\nHow it works Capmon uses BPF tracepoints and fexit tracing to listen to two different events.\nCapability checks (fexit) Process starts (tracepoint) (1) looks a the return value of the capability check. By default it listens to the ns_capable and capable_wrt_inode_uidgid kernel functions. Most required capability checks go through either of these two functions. But in case it doesn\u0026rsquo;t, the --all flag changes it to instead listen to the cap_capable. The capability and the result is saved and mapped to the PID that did the check.\n(2) looks at the PID of the parent process (the one who started the new process) and if that matches with the PID of the command it is saved. Next time a process starts it will look against all previously saved PID. This ensures that any even subprocesses of subprocesses, and so on, are kept track of.\nAt the end it compares all saved PID (the process and its subprocesses) against all capability checks done and takes the intersection. The output is the capability checks done by the input command.\nCapmon itself ignores Ctrl-C (SIGINT) so it is passed down to the monitored program. This allows it to support interactive programs that are stopped with Ctrl-C.\nIt currently does not handle orphan processes since it stops once the initial command is done. To handle this there is monitor mode using the --monitor flag. This mode does not run any command, instead it behaves similar to tcpdump and can filter and output a summary based on PID or name.\n","permalink":"https://casan.se/docs/capmon/","summary":"Monitor when processes check capabilities(7) to find out what they require. Run capmon '\u0026lt;cmd\u0026gt;' to track the capabilities it requires. It\u0026rsquo;s important to not run it with sudo since that bypasses certain checks. Let the command fail at the first missing capability check and add that capability, then try again and see if if fails on more. Rinse and repeat until command runs successfully.\nIf you are not familiar with capabilities can read A simpler life without sudo.","title":"Capmon"},{"content":" Computers in networks traditionally don\u0026rsquo;t have any knowledge of when other computers can/will transmit data. Dozens of devices transmitting data whenever they want will inevitably lead to collisions and congestions at one point or another, which in turn leads to packets not arriving as soon as they should. Packets arriving later than intended may affect what the receiver should do with it. Data can expire. If the timeframe for its usefulness has passed then the receiver must know this, or else it may perform actions based on old information. There exist techniques to improve predictability in networks, but this post will focus on a common understanding of time.\nClock synchronization across the internet typically uses software timestamping to make sure your computer clock shows the same as the rest of the world (disregarding timezones). In a best-case scenario software timestamping can reach a precision of under a millisecond, but also a worst-case of hundreds of milliseconds. To the human eye looking at a clock, this appears good enough, but machines may require much higher precision in certain use cases.\nA new level of precision The IEEE 1588 standard defines the Precision Time Protocol, also known as PTP. This protocol allows synchronization of hardware clocks down to nanosecond precision in a local network. Devices achieve this by timestamping packets they send and receive and comparing the time, and estimating and adjusting according to the cable delay. But using software timestamping still limits the precision due to processor scheduling and other network traffic that creates variation in the actual transmission time.\nTo solve this there exist ethernet ports with support for hardware timestamping. PTP requires a device to know both the transmission and reception time. Sending over only the current time from the master clock would make the slave show an earlier time than the master since the master will have moved forward by the time the slave has received and adjusted the clock. The participating devices also need to determine how long delay the wire itself incurs. A long cable, or even a time-unaware switch, can add noticeable delays (noticeable from a nanosecond perspective).\nGood! Now we have established the requirement! But how does this help us?\nThe synchronization process of a slave clock consists of two steps:\nGetting the current time from a master clock Finding the cable delay and adding that to the time received from the master Step (1) involves the master sending the current time, taken as close to the wire as possible, in a Sync packet to the slave. PTP defines two methods for doing this: one-step and two-step. This is explained in more detail later. The slave can now update its clock to the provided timestamp. Upon reception, the packet gets timestamped again. Now the slave has receive_time - transmit_time = offset_from_master. The slave can now adjust its clock to the calculated offset. Though it has not yet compensated for the cable delay. If PTP only ever did step (1) the slave would always stay behind the master by cable_delay time. Devices would then end up further behind the further they are from the master, and each intermediate switch would add more to that.\nFor step (2) the slave sends a Delay_Req (delay request) to the master and records the transmission time (t1) for itself to use later. The master timestamps the reception time (t2) of the message and sends that back in a Delay_Resp (delay response) to the slave.\nBecause t1 was already cable_delay time behind the master clock, due to the cable delay on the previous Sync packet, the difference between t1 and t2 will now be cable_delay * 2. This means the slave clock should add (t2-t1)/2 to its clock. Now the master and slave clocks have successfully synchronized. The process will then repeat at regular intervals to make sure everything stays synchronized.\nSoftware daemon The hardware only needs to implement the timestamping functionality to support PTP. But managing these different types of packets, as well as deciding who should be master and who should be slave, requires software. Usually in the form of a daemon. Richard Cochran maintains the most popular implementation of PTP in the project linuxptp.\nOne-step vs two-step When the hardware receives a packet it will save the timestamp in packet metadata that can then be fetched by the receiving application. Simple enough!\nThe transmission poses more of a challenge since metadata can\u0026rsquo;t be included on the wire. As mentioned earlier, there exist two methods for handling this. The simplest one involves sending a packet, taking the timestamp from when it was sent, and then sending a Follow_Up packet that includes the transmission time of the first packet. This is called two-step timestamping.\nThe other alternative, one-step timestamping, requires hardware that can detect and modify the right fields in the PTP packets as they go out on the wire. That way the packet contains the data.\nThe following illustration shows the slave clock synchronizing to the master clock using two-step timestamping. The cable has a delay of 1 time unit. Described from the point of the master clock\u0026rsquo;s time:\nMaster Slave ┌───────────┐ │ │ 50─┼──────┐ ├─20 │ │ │ 51─┼────┐ └───►├─21 │ │ │ 52─┤ └─────►├─22-\u0026gt;51 │ │ 53─┤ ┌─────┼─52 │ │ │ 54─┤◄────┘ ├─53 │ │ 55─┼─────┐ ├─54 │ │ │ 56─┤ └────►├─55-\u0026gt;56 │ │ 57─┤ ├─57 └───────────┘ Master sends Sync packet and timestamps it (50). Slave receives Sync and timestamps it (21). Master sends Follow_Up containing the transmission time of Sync. Slave receives Follow_Up and calculates the difference between Sync transmission and reception. 50-21=29. Slave updates its clock by adjusting it +29. 22+29=51. Slave sends a Delay_Req and timestamps it (52). Master receives Delay_Req and timestamps it (54). Master sends back the Delay_Req timestamp in a Delay_Resp packet. Slave receives Delay_Resp. It now has the timestamps 52 and 54, which represents the cable delay multiplied by 2. Half comes from the packet delay request. And the other half comes from the earlier Sync packet where the slave knowingly set its time to cable_delay behind the master since it didn\u0026rsquo;t know the delay. The slave adjusts its time by (54-52)/2=1 and moves it 1 unit forward. The clocks are now synced. Using one-step works the same, with the only difference that the master does not need to send a Follow_Up packet.\nTwo-step only requires the networking hardware to be capable of timestamping packets. The timestamping happens either in the MAC hardware or the PHY hardware. The PHY will provide better accuracy since it allows timestamping to be the last action before the packet goes on the wire. Performing timestamping in the MAC can give slightly higher variation in accuracy, but still good enough for many use cases.\nThe following illustration shows an example layout of how a MAC and PHY would be attached in switching hardware. The PHY attaches directly to the cable.\n┌───┐ │CPU│ └─┬─┘ │ ┌─┴─┐ ┌───┤MAC├───┐ │ └─┬─┘ │ │ │ │ ┌─┴─┐ ┌─┴─┐ ┌─┴─┐ │PHY│ │PHY│ │PHY│ └▲─▼┘ └▲─▼┘ └▲─▼┘ For one-step, the hardware also needs the functionality to modify the packet, and that includes understanding the PTP packet layout. Using one-step results in fewer packets to handle and slightly faster convergence (time for all clocks in the network to get accurately synced) since it never has to wait for a second packet. Though with the introduction of the standard 802.1AS, it has been shown that two-step can perform just as well, and the extra traffic on the network becomes insignificant at the network speeds of today.\nSo why doesn\u0026rsquo;t everyone use one-step? At the surface, it looks good, but looking deeper one-step has some drawbacks. When reaching speeds of 10Gbit and higher it will incur penalties for the time spent taking the timestamp and modifying the packet1. The network transmits fast enough that it takes longer to add the time than the actual transmission. This means that the hardware can\u0026rsquo;t work at full wire speed while timestamping just before transmitting. To get around this the hardware could try to predict the transmission time and prepare the packet ahead of time. But that too has its issues as preparing ahead of time adds latency to all outgoing packets on that port. A not-so desirable trait in time-sensitive networks.\nhttps://www.ieee802.org/1/files/public/docs2015/ASRev-pannell-To-1-step-or-not-0315-v1.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://casan.se/blog/networks/ptp-and-timestamping-methods/","summary":"Computers in networks traditionally don\u0026rsquo;t have any knowledge of when other computers can/will transmit data. Dozens of devices transmitting data whenever they want will inevitably lead to collisions and congestions at one point or another, which in turn leads to packets not arriving as soon as they should. Packets arriving later than intended may affect what the receiver should do with it. Data can expire. If the timeframe for its usefulness has passed then the receiver must know this, or else it may perform actions based on old information.","title":"PTP and timestamping methods"},{"content":"This is just a short note to myself and anyone who encounters the same issue. While writing a post recently I wanted a numbered list that started with a number other than 1. Frustratingly, Jekyll does not support this. Jekyll always start lists with 1. I found this very inconvenient and tried to find a way around it. One solution is inline HTML. But then you are restricted to writing lists in HTML, and everything inside the lists needs to be formatted with HTML as well, if you want italics or code blocks, etc.\nAs I found out, Jekyll supports adding custom classes, IDs, and properties to HTML objects. To start my list at a different number I could simply do.\n{:start=\u0026#34;5\u0026#34;} 1. First item 2. Second item 3. Third item And the result is:\nFirst item Second item Third item Note that the {:} requires a blank line before it when styling a whole block, but it can also be inlined to apply to elements within a text. Classes and IDs can be added with {:.myclass} and {:#myid}. For more details on this I refer you to the place where I found it: Jekyll Tip: Adding Styling To Html Output\nAccording to the CommonMark markdown specification the rendered list should start with whatever the first number may be (within some restrictions, such as it can\u0026rsquo;t be negative and maximum 9 digits). The following numbers are disregarded and will only count up from the initial list item. Refer to 5.3 Lists and examples #265, #267, and #268.\n","permalink":"https://casan.se/blog/misc/jekyll-markdown-lists-that-dont-start-on-1./","summary":"This is just a short note to myself and anyone who encounters the same issue. While writing a post recently I wanted a numbered list that started with a number other than 1. Frustratingly, Jekyll does not support this. Jekyll always start lists with 1. I found this very inconvenient and tried to find a way around it. One solution is inline HTML. But then you are restricted to writing lists in HTML, and everything inside the lists needs to be formatted with HTML as well, if you want italics or code blocks, etc.","title":"Jekyll markdown lists that don't start on 1."},{"content":"In my daily work I sometimes want to copy and modify raw frames as hexdumps. I will usually copy a hexdump from Wireshark or Tcpdump. Some times I have, however, written frames from scratch. Not very complex ones; nevertheless, it is nice to have the possibility to.\nThere are plenty of tools out there for building and sending frames. Two good ones I use are Nemesis and EasyFrames. They provide many options for creating different types of frames. Though sometimes you just want full freedom and not have to think about how to do something with the tool. I did not find any complete project that did this. I spent a lot of time googling. I found an easy way to send raw hex frames in Python on StackOverflow that I used for a bit. Eventually I decided to make it into a proper application. I wanted it in C, with the reason being that I might want to use it in an embedded enviroment where Python is not available. I also considered if Bash or any other common shell commands could do this, but nothing that I came up with. Most did not support raw frames. Otherwise, it would have been really sweet to have it as a simple shell script.\nHexend provides an easy way to repeatedly send any frame you want. A specific type of frame is causing you issues? Don\u0026rsquo;t bother with pcap playbacks or recreating the scenario to trigger the problem. Copy the hexdump (don\u0026rsquo;t include any line numbers) and start sending the frames. You can specify an amount of frames and an interval for sending them.\nHexend is also scriptable; it supports piping the frame to stdin. However, I\u0026rsquo;m not entirely sure of any use-case for this yet. Normally you will probably place the hexdump in a file and pass the filename as an argument to Hexend.\nHere is an example of piping a minimal frame (dst MAC, src MAC, EtherType) to hexend that gets sent to eth0 1000 times with no interval.\necho ffffffffffffaaaaaaaaaaaa0000 | hexend eth0 -c 1000 -i 0 Edit:\nI found a way to do it with shell scripting, depending on xxd and socat. The performance isn\u0026rsquo;t great for sending many frames quickly. But it is able to use the same input methods. I added a script file to the repo as an alternative. But all it does right now is just this:\ncat $2 | xxd -r -p | socat -u STDIN interface:$1 ","permalink":"https://casan.se/blog/networks/hexend-send-raw-hex-frames/","summary":"In my daily work I sometimes want to copy and modify raw frames as hexdumps. I will usually copy a hexdump from Wireshark or Tcpdump. Some times I have, however, written frames from scratch. Not very complex ones; nevertheless, it is nice to have the possibility to.\nThere are plenty of tools out there for building and sending frames. Two good ones I use are Nemesis and EasyFrames. They provide many options for creating different types of frames.","title":"Hexend - Send raw hex frames"},{"content":"This is a short story from $DAYJOB about my discovery and investigations of an issue with the build system.\nOne day when building our code I suddenly got a segmentation fault when building. That\u0026rsquo;s odd! I could not recall any changes to the build system and checking the Git log I could not find any changes that could possibly affect it. The mystery grew as I checked out older Git commits and would still have the crash. I managed to narrow the issue down to one package which was being built slightly differently than other packages. Still, it didn\u0026rsquo;t make sense to me why it was failing.\nWhat confused me the most was that it was failing between two steps in the build process. I added debug prints to the build system and I could see that it finished the configuration step, but never reached the start of the build step. I did not dig deeper into the build system here. At this point, I suspected there was some issue with GNU Make. It output a core dump upon crashing, so my next step was to take a look at that core dump. Find out how it is crashing.\nThe core dump led me to a function called func_filter_filterout, which had a loop that looked like this.\nwhile ((p = find_next_token (\u0026amp;word_iterator, \u0026amp;len)) != 0) { struct a_word *word = alloca (sizeof (struct a_word)); *wordtail = word; wordtail = \u0026amp;word-\u0026gt;next; word-\u0026gt;str = p; // ... The crash would occur on the last line, but the key here is the function call to alloca. It allocates memory similar to malloc, with the difference that the allocation happens on the stack. This allows the memory to later be freed automatically when the function exits. Reading the manpage alloca(3) I found the following.\nBUGS There is no error indication if the stack frame cannot be extended. (However, after a failed allocation, the program is likely to receive a SIGSEGV signal if it attempts to access the unallocated space.) To verify that this was what I saw I printed the address of word. Doing so I could see the address going up and down a lot, indicating it was allocating for a while before returning the function and freeing, then being called again. When it reached the point where Make would crash I noticed the address increasing continuously for a while and eventually crashing.\nCurious as to why it was stuck in a seemingly infinite loop I changed my print to output p, the strings being iterated over. The output showed Make variables for many packages. Even packages not in use. It did not make sense why Make would iterate over them when building a completely unrelated package. It did not do this iteration at all when building other packages.\nFor some reason, Make would start iterating over all variables. I did not delve further into this right now, though I probably should have. I switched focus. Why did it start happening now? Why was it never an issue before?\nIt didn\u0026rsquo;t take too long for me to realize that a couple of weeks prior I had upgraded Ubuntu on my work laptop from 21.04 to 22.04. Could they have upgraded the Make version? Nope, Make 4.3 was released about 3 years ago and has not had a new release. I talked to a colleague who was still on Ubuntu 21.04 and we both had the Make 4.3. He sent me his Make binary for me to test. Surprisingly, it works fine. Make 4.2.1 from Ubuntu 18.04 also works fine. It is not the environment that is the issue. Something with the Make version shipped with Ubuntu 22.04 is different. Well, maybe Ubuntu has some internal patches to Make that introduced the bug? Let\u0026rsquo;s try building Make 4.3 from source!\nCRASH! What!? The issue exists on the original release too?! Had Ubuntu fixed this previously and now removed the fix? Are there differences in how it\u0026rsquo;s built? Even more questions I have yet to find an answer to.\nTesting it with upstream Make did not cause the crash, but a lot has been refactored since the last release of Make.\nAt this point, I turned away from chasing Make. At least using upstream works fine. Being forced to build with tools from older Ubuntu versions would be awful in the long run. It could end up with the build system using a Docker container with Ubuntu 21.04 to build it for many years in the future.\nI turned my attention back to the \u0026ldquo;faulty\u0026rdquo; package in our build system. Since it was never really handled in an ideal way I decided to dig into that instead.\nOnly after doing this did I discover this piece of Make code: $(.VARIABLES) that was used as input to the filter function, which made sense given that it matched with the name of the function responsible for the segmentation fault. I should have looked closer at this code earlier, but at a glance the code around it didn\u0026rsquo;t do anything groundbreaking. It turns out this is a special variable in Make that holds all other variables\u0026rsquo; names, and it was being iterated over.\nI still have not found out why it suddenly became an issue on Ubuntu 22, but at least I found what caused the issue. This journey has been a very interesting and educational one. Build tools are the last thing you expect to crash when writing code.\n","permalink":"https://casan.se/blog/programming/the-day-make-started-crashing/","summary":"This is a short story from $DAYJOB about my discovery and investigations of an issue with the build system.\nOne day when building our code I suddenly got a segmentation fault when building. That\u0026rsquo;s odd! I could not recall any changes to the build system and checking the Git log I could not find any changes that could possibly affect it. The mystery grew as I checked out older Git commits and would still have the crash.","title":"The day Make started crashing"},{"content":"An incredibly underused but oh-so-amazing feature in Linux is capabilities. Everyone should use it to make their lives simpler.\nBy default, a user does not have many permissions on a system. Some are granted automatically through different means, such as reading and writing files in your home directory. As developers, we may regularly have to reach for sudo to allow us to do what we want. Writing our password over and over again, or constantly forgetting to prefix your command with sudo. It can get tiring.\nOn the other end, you could do like ye olden days and start a shell as a superuser; run everything with sudo. However, sudo was specifically invented to not have users run everything as a superuser, as it is inherently a bad idea. Using sudo gives you access to everything. Not using it gives you access to very little. But what if there was a middle ground?\nEnter capabilities. All of the actions that normally require sudo have one or more capabilities associated with them. Sudo grants you all capabilities, allowing for any kind of mayhem when running commands with it. However, you can also grant yourself only some of these capabilities. It is assigned per user and application. This means that to run a command that normally requires sudo without it, both the user and the command must have the required capabilities. This gives the bonus of safety, an application cannot do actions you explicitly haven\u0026rsquo;t given it access to. It also allows system administrators to give users access to run certain applications without the need to give them sudo rights.\nUsing capabilities As mentioned above, capabilities are given to both users and applications. Let\u0026rsquo;s start with users. This is stored in the file /etc/security/capability.conf. Edit the file and add a line like this but with your username.\ncap_net_raw,cap_net_admin\tcasan This gives the user casan the two capabilities cap_net_raw and cap_net_admin and should take effect immediately. Next, we need some application. Let\u0026rsquo;s use the ip command. To find where the command is located we can use which ip. An important detail here is that you can\u0026rsquo;t give capabilities to symlinks, so if a command is symlinked the capability must be given to the actual executable file.\nBut before we add the capability, let\u0026rsquo;s try doing a command without sudo. It should fail with ioctl(TUNSETIFF): Operation not permitted.\nip tuntap add tap99 mod tap Now add the capability cap_net_admin to the command.\nsudo setcap cap_net_admin+ep /bin/ip Try the previous command again and it should work. And just to clean up our mess we can delete the newly created tap99 interface with ip link del dev tap99 (which we can also do without sudo).\nNow you know how to add capabilities to avoid using sudo. But how do we know which capabilities a command needs? It can depend on which arguments you pass the command, depending on what the arguments do. If you use the commands of ip netns you need different capabilities than the example above. You can always read the manpage capabilities(7) to get a better understanding of the individual capabilities. But if you don\u0026rsquo;t fully understand how a command works it can be difficult to figure out. It can also be the case of one command starting another subprocess, where the subprocess is the one that needs the capability.\nCapmon: figuring out capabilities Introducing Capmon - a Linux capabilities monitor. A tool that monitors your process and shows a report over which capabilities it looked for. You simply pass your command as an argument to Capmon and it will execute it. But before you get started, Capmon does require the capabilities cap_dac_override and cap_sys_admin, running it with sudo does not work correctly because that bypasses certain checks. Now that you\u0026rsquo;ve added those capabilities to yourself and Capmon, let\u0026rsquo;s try it out.\ncapmon \u0026#39;ip tuntap add tap99 mod tap\u0026#39; Assuming you did everything correctly, this should output the following text indicating that the ip application accessed the cap_net_admin capability successfully.\n[ip] - [PASS] CAP_NET_ADMIN Now take what you\u0026rsquo;ve learnt here and use more commands without sudo.\nSome things to keep in mind If a command is failing but you are not seeing any FAIL output from Capmon, try running with the flag -a. This adds some additional monitoring points. It is recommended to use quotes around your command, otherwise dash-arguments arguments will be interpreted as arguments to Capmon. Commands that require multiple capabilities will usually stop and return an error on the first failed check. In this case, add the first capability, then run it again and it will fail on the second. Footnote The title of this post is a homage to a blogpost by a former colleague who taught me about capabilities and a ton of other things, which in turn inspired me to create Capmon. You can read his post at A life without sudo.\n","permalink":"https://casan.se/blog/linux/a-simpler-life-without-sudo/","summary":"An incredibly underused but oh-so-amazing feature in Linux is capabilities. Everyone should use it to make their lives simpler.\nBy default, a user does not have many permissions on a system. Some are granted automatically through different means, such as reading and writing files in your home directory. As developers, we may regularly have to reach for sudo to allow us to do what we want. Writing our password over and over again, or constantly forgetting to prefix your command with sudo.","title":"A simpler life without sudo"},{"content":"Let\u0026rsquo;s dive a little deeper into Time-Aware Shapers, specified in the IEEE 802.1Qbv standard. For a light introduction go read my intro to TSN.\nGates The shapers are applied on egress and each shaper has a list of time slots, where each time slot specifies a duration, operation, and gate_mask. The duration represents how long the time slot is. The gate_mask specifies which egress queues are open during that time. Multiple queues can be open at the same time, at which point it treats it as regular Strict Priority between those queues, i.e., higher priority goes first. operation will be covered later. Time-Aware Shapers can have a theoretically infinite number of these time slots, also known as gates, but in practice there will of course be a limit to what both the software and hardware can support. The sum of the gates\u0026rsquo; durations defines the cycle time. When it has iterated through all the gates it restarts with the first gate again. Below is an example of a schedule with two gates, one which opens priorities 0-3 and one which opens priorities 4-7.\n|---------|---------|---------|---------| | 0,1,2,3 | 4,5,6,7 | 0,1,2,3 | 4,5,6,7 | ... |---------|---------|---------|---------| You should always make sure that all possible priorities in your system should be able to send at least during one gate. Otherwise it will take up space in the queue and never be sent. You can of course use only priorities 0-2 if you wish, but then you must make sure that no priority is ever mapped to the rest. The priority is usually the PCP or DSCP value, but how you map that to the internal priority of the switch is up to you. Since DSCP can have 64 different values you already need some sort of reduced mapping to the 8 queues used by the shaper.\nThe lower limit of what defines reasonable durations is that frames need to still be able to pass through with little interruption. Too short and it will start causing more problems that it solves. With too short gates it could result in frames either not fitting in the gate and start encroaching on the next time slot, or being preempted a lot if used with Frame Preemption.\nThe maximum latency for your requirements defines the upper limit of a single gate\u0026rsquo;s duration A low-priority gate cannot have a duration longer than the maximum latency, otherwise the higher priorities has to wait too long for their turn. The total cycle time has no theoretical upper limit, but at some point it will be more worth to have it cycle back to the start instead of defining more gates.\nAll gates have a guard band at the end of their duration. During the guard band no new frames can be sent, it only allows already started frames to finish. This is to avoid frames encroaching on the next time slot. To completely avoid it you can set the guard band to the maximum size of a frame, 1518 bytes. This means that if the frame started sending it will be guaranteed to finish before the gate closes. However, this could end up wasting a lot of time when a frame finishes very early into the guard band and it has to idle until the next gate.\n|------------------|-------------| | Gate 1 | GB | Gate 2 | GB | |------------------|-------------| Frame Preemption Frame Preemption can be used in conjunction with Time-Aware Shapers to make them more efficient. Frame preemption was covered briefly in the TSN introduction post. A quick recap. It requires frames to be classified as express or non-express frames. It allows non-express frames to be interrupted by express frames to allow the express frame to send immediately.\nWhen used with Time-Aware Shaper you apply the different Frame Preemption options SetGateStates (S), Set-And-Hold-MAC (H), and Set-And-Release-MAC (R) to the different gates. S does nothing with respect to Frame Preemption, but is an indicator that the guard band needs to be longer before it starts. H makes sure any frames from previous gate are preempted when the gate starts. R allows non-express frames to send again. H allows the guard bands to be reduced drastically because now they can preempt a frame when reached. It only needs to add the extra CRC checksum and then it can move on to the next gate. Shorter guard bands enables better bandwidth usage.\nIf you have multiple express gates (H) in a row then it won\u0026rsquo;t be able to preempt in preparation for the next gate, but then you also have bad design :).\n","permalink":"https://casan.se/blog/networks/tsn-time-aware-shaper/","summary":"Let\u0026rsquo;s dive a little deeper into Time-Aware Shapers, specified in the IEEE 802.1Qbv standard. For a light introduction go read my intro to TSN.\nGates The shapers are applied on egress and each shaper has a list of time slots, where each time slot specifies a duration, operation, and gate_mask. The duration represents how long the time slot is. The gate_mask specifies which egress queues are open during that time. Multiple queues can be open at the same time, at which point it treats it as regular Strict Priority between those queues, i.","title":"TSN: Time-Aware Shaper"},{"content":"Disclaimer! I am in no way an expert at this. I\u0026rsquo;m learning it as part of a research project at work. This will be the first post in what I hope to be a series of posts on TSN.\nWhat is it? Time-Sensitive Networking (TSN) is a topic in the networking world where there exists data frames that absolutely must arrive within a limited time. It is a fairly new topic and some of the techniques are not completely industry-tested yet. It originates from the sound and video world where they have sound and video that must arrive very quickly. It was originally called Audio Video Bridging (AVB), but was renamed to Time-Sensitive Networking (TSN) in 2012.\nIt could be used at concerts, for example. The sound data must to travel from the microphones to all the speakers, and maybe some cameras to stream to the screen; and it all has to happen very quickly. Humans are very sensitive to delays in sound and video when it doesn\u0026rsquo;t match up with the reality. To make this even more complex there is often other types of data travelling the network as well. You could make the argument that the sound should use its own cables; however, that is extra maintenance and requires a separate network to run on. It is convenient to be able to use the same network for everything.\nTo work around this they have come up with several techniques for handling Quality of Service (QoS) in networks. I will cover some QoS techniques now, and future posts will go more in-depth on some of them. These are all defined in IEEE standards.\nTSN techniques Strict Priority (802.1Q) The most basic form of QoS. This, as well as several of the below techniques, are based on the concept of multiple egress queues. Usually 8 of them. With 8 queues you can do a 1-to-1 mapping from the Priority Code Point (PCP) in the VLAN tag. There are some techniques to set the priority from software (see man mqprio). In Linux, this is the skb-\u0026gt;priority field. But when you use QoS you usually want the frames to only pass through the hardware. There is currently only support in the Linux kernel to set this for this for the DSCP field (man dcb-app), but hopefully we can get a proper way to set this for PCP as well. It is already supported by a lot of the hardware. There are other ways to do this in the hardware as well, but not all are ideal.\nAs for the algorithm itself, it is very simple. It transmits frames in order of priority. When it starts transmitting a frames it picks from the highest priority queue that is not empty. A lower priority queue will only transmit if all higher priority queues are empty. Below is an illustration of 3 transmit queues with frames A-H, where frames in queue 2 would have the highest priority.\nQueue ---------------------- 0 -\u0026gt; [H] [G] [F] [D] -\u0026gt; ---------------------- 1 -\u0026gt; [E] [C] -\u0026gt; ---------------------- 2 -\u0026gt; [B] [A] -\u0026gt; ---------------------- Time-Aware Shaper (802.1Qbv) This extends the Strict Priority algorithm by adding a time aspect to it and splitting the time into several slots where different priorities are allowed to send in each slot. For example, it could have 2 slots, one where only priority 7 (highest) is allowed to send, and another where priority 0-6 are allowed to send. The time slots can be of different length. For the second slot there are 7 different priorities that can send, so between those it applies Strict Priority.\nTime-Aware Shapers can run in software, but the benefit it gives is completely lost. To be at all useful it needs to be offloaded to hardware, which also requires a PTP hardware clock. On top of the clock itself it should also be time synced to all other switches in the network. Otherwise, there is also very little point in using it. If all switches are synced it will result in the same priorities all being open at once in the network; ideally letting the frames flow all the way to their destination without being interrupted by other traffic. To put it simply, it can reserve bandwidth for the different priorities and giving the network a deterministic behavior (at least for high priorities), but of course this assumes you aren\u0026rsquo;t overloading the network itself. If you try to overload the link it will obviously drop and delay frames.\n[Read more\u0026hellip;]({% post_url networks/2022-08-25-time-aware-shaper %})\nCredit-Based Shaper (802.1Qav) The original TSN technique that was invented for the purpose of audio and video. It reduces the congestion in the network by evening out bursts. It is a form of fair queueing. Like Strict Priority, this also uses multiple egress queues. Each queue holds a credit score that goes up when a queue is not transmitting, and down when it transmits. When the credit score goes negative it is not allowed to transmit. By evening out the traffic to get rid of bursts the data can flow more freely in the network. This is especially useful when there are many devices in the network that may send bursts. If all send a burst at once it can result in the queues filling up where the bursts meet.\nFrame Preemption (802.1Qbu) Frame preemption is the only of the priority-based TSN techniques that requires both ends of a link to support its protocol. The ones above can all operate on a single switch, though ideally used in a network were all are running the same technique. It also requires hardware support. With frame preemption both sides work together to provide faster delivery speed. To begin, we first have to define which priorities (queues) are to be considered preemptible. This is done independently on each switch.\nIf a preemptible frame is currently being transmitted and an express (non-preemptible) frame arrives to the queue it will almost immediately stop the preemptible frame (it preempts it). To not have wasted all the work already done sending the frame it appends a CRC32 checksum to the end of the partial frame, as well as inverts the last 16 bits of the CRC value to distinguish a partial frame from a regular frame. When that is done it can start sending the express frame. The partial frame is stored in a separate buffer on the other end and waits for the other half of its frame to arrive. The other half is sent when no more express traffic is available to send.\nFrame Replication and Elimination for Reliability (802.1CB) The final technique for this post is the only one not dependent on multiple queues and priorities. Rather than providing fast delivery it is a form of redundancy. The idea is that it duplicates frames and sends them different paths in a ring or otherwise redundant topology. In the case of one switch failing it doesn\u0026rsquo;t even have to wait for the switches to learn the new path. The frames are already on their way the other path as well. To not get duplicate frames at the destination it eliminates any duplicates where they meet again, ideally as close to the destination as possible; possibly even at the end device. Same as frame preemption, this requires the mergeing device to also support the protocol. The intermediary switches do not need to support it, they just need to forward the frame.\nKeeping track of duplicates in the merging device is done with the help of a sequence number in the duplicated frames. This is called the R-tag and it has a 16-bits field to store the sequence number. The R-tag is then removed when a frame arrives at the merging device, and any following frames with the same sequence numbers are discarded.\nIn the case of multiple senders to the same merging device it can employ a stream identification function to identify which data stream a frame belongs to. Two frames can have the same sequence number but belong to different streams, and we don\u0026rsquo;t want one of those to be discarded because they are not the same frame.\nTSN today TSN techniques are still considered fairly new and is not entirely industry-proven. It is starting to appear more and more in factories with the advent of Industry 4.0 and companies wanting to make their factories smart and more modern. By using a single network for everything you simplify connections and reduce the amount of cables. The time-sensitive aspect comes from, for example, control systems that need to send instructions instantly other machines. It has also gained traction in the automobile industry for use inside cars to connect various smart devices such as cameras, sensors, control devices, etc., with the main computers.\n","permalink":"https://casan.se/blog/networks/what-is-time-sensitive-networking/","summary":"Disclaimer! I am in no way an expert at this. I\u0026rsquo;m learning it as part of a research project at work. This will be the first post in what I hope to be a series of posts on TSN.\nWhat is it? Time-Sensitive Networking (TSN) is a topic in the networking world where there exists data frames that absolutely must arrive within a limited time. It is a fairly new topic and some of the techniques are not completely industry-tested yet.","title":"What is Time-Sensitive Networking?"},{"content":"A network switch has multiple ports to send and receive data on. To manage this there is a component called \u0026ldquo;bridge\u0026rdquo; inside it. In Linux the bridge is defined as a network interface, but it it can also be offloaded to hardware (at least parts of it). The bridge is responsible for connecting different networks with each other. A packet coming in on one connection has the possibility to go out on any of the other bridged connections.\nForwarding Database (fdb) The Forwarding Database (fdb) stores the information about which bridged connection a data packet should use depending on what the destination is. An fdb entry needs to store the MAC address of the known destination and which connection it should use to reach that. The connection is specified as an interface, e.g., eth3 or vlan3. When a packet arrives at the bridge the MAC address will be inspected and matched to the fdb. If an entry is found it will be forwarded to that interface. If none is found it is treated as unknown and will be flooded to all ports of the same bridge (with the exception for any ports that have flooding disabled).\nHopefully, the flood will result in the destination responding to the packet, sending a reply back to the source. When the reply passes through the bridge it sees which interface it arrived on and will save that in the fdb for next time. Now packets have passed through the bridge from both devices and it has learned which interface each destination is on. Next time they communicate the bridge will find the entries in the fdb and forward the packets correctly.\nShow the contents of the fdb in Linux\nbridge fdb show Ageing To avoid the fdb filling up with old entries a bridge can use ageing. It keeps count of how many seconds has passed since each MAC address sent a packet through the bridge; the counter is reset when a packet from that MAC address enters the bridge. When the age reaches a set threshold it will be removed from the fdb. For example, if a device stops responding to requests it will never refresh the age and the fdb entry will eventually be forgotten since the device is no longer considered part of the network.\nWhat if you want to have a device that only listens to requests? Easy! You can get around the ageing by setting a static fdb entry. These entries will never age, they stay there until you say otherwise.\nAdd a MAC address to be forwarded to the specified interface. To make it static you simply add static at the end\nbridge fdb add \u0026lt;MAC\u0026gt; dev \u0026lt;INTERFACE\u0026gt; Multicast database (mdb) The kernel also stores a database of all the multicast groups, which can be seen if you run\nbridge mdb show For multicast it will create a entry when it gets an IGMP Join message. It creates a entry for that group and marks the port that it arrived on. The goal of multicast is to be able to send one message to multiple recipients. Which means that the mdb must store all ports that are in the group, and then remove them when they leave. This is typically stored in a port masks. For a switch with 8 ports, where ports 2 and 4 are in the group, it could look like this 00001010 (represented by the second and fourth bit). This indicates the second and fourth port are in the group. When all ports eventually leave the group it can remove the entry.\nHardware offloading Sometimes you want a bit higher performance. For those times you can use a specialized hardware circuit for network switching. You get less load on your CPU and higher throughput because the packets are switched directly in the hardware and never touches the CPU. One drawback of this is that the hardware only has the features it was built with. If you ever lack some feature you can\u0026rsquo;t simply write some code for it. But it usually comes with at least some common features, such as its own fdb and mdb that are stored in its own internal memory.\nThe fdb and mdb are stored in a table with fast lookup to quickly switch the packets. The table might look something like this (simplified, it will usually contain more information, e.g., age and if it is static or not)\n------------------------------------------------- | MAC | VLAN | TYPE | DESTINATION | |-----------------------------------------------| | 00:00:00:00:00:01 | 1 | UC | 2 | | 00:00:00:12:ad:fd | 2 | UC | 5 | | 01:00:5e:01:02:03 | 2 | MC | 1 | ------------------------------------------------- The type indicates Unicast or Multicast. The unicast entries points at the port it should send the packet to. Multicast, on the other hand, points to an index in another table. This is where the port mask is stored.\nEntries to this table can be added if the device driver supports it. Then an entry will be added to both software and hardware, since it goes through software first. All multicast addresses are added through software because that is where the IGMP protocol is handled. Though, some entries may exist in hardware only. This happens when the hardware learns unicast entries.\nThese are all things the bridge manages, whether it is a software defined bridge ,like the one in Linux, or if it has been offloaded to a hardware circuit. This has been a little summary of what I have learned at work recently. Thanks for reading!\n","permalink":"https://casan.se/blog/networks/network-bridge-forwarding-and-learning/","summary":"A network switch has multiple ports to send and receive data on. To manage this there is a component called \u0026ldquo;bridge\u0026rdquo; inside it. In Linux the bridge is defined as a network interface, but it it can also be offloaded to hardware (at least parts of it). The bridge is responsible for connecting different networks with each other. A packet coming in on one connection has the possibility to go out on any of the other bridged connections.","title":"Network bridge forwarding and learning"},{"content":"In a previous post I talked about deterministic functions and some useful properties they have. I recommend you read that one first if you haven\u0026rsquo;t already. But deterministic functions aren\u0026rsquo;t all that useful to get tasks done. They only transform data from one form to another, but cannot actually read input from the console or even print to the console. A program consisting of only deterministic functions will always perform the same calculation, and given that it doesn\u0026rsquo;t have input or output it will only use whatever numbers it was compiled with. It essentially becomes a black box that does some computation without ever showing the result. It is just a waste of time. To do anything useful we need input and output (IO) in one way or another. This doesn\u0026rsquo;t mean every function should be nondeterministic. I still encourage you to make as much as possible deterministic, for the reasons outlined in the previous post. But IO is a must for any useful program.\nTypes of nondeterminism Nondeterminism comes in many forms. There were 3 main ones that I could think of, that I will talk about here. Though you might be able to find more, and probably different subcategories of each of them as well.\nGlobal state means to change the values of variables that live in the global scope, or any scope that is outside the function. Static class variables are also included here. The way it creates nondeterminism is pretty obvious. Changing state will make the function behave differently the next time. Modifying a reference is another way to change the state. References are a pointer to a variable. If you pass a reference to a variable you can then modify the original. The function below demonstrates nondeterminism through a global variable.\nx = 10 def f(y): global x x += 1 return y + x print(f(5)) # prints 16 print(f(5)) # prints 17 IO is also the obvious one which I\u0026rsquo;ve already mentioned. Reading from or printing to console, reading/writing files, http requests, database requests. Anything that interacts with the world outside of the program itself. This is because everything outside the program is subject to possible changes at any time (from the program\u0026rsquo;s perspective). The data in the database may change. The file it reads may have changed. Random values is a subcategory of IO since random numbers need to be seeded from the operating system, unless you want the same random sequence every time you execute. Random values will therefore also contribute to nondeterminism.\nExceptions interrupt the flow of the program when they occur. Exceptions as a concept is a problem for determinism since the program no longer executes as expected. Zero-division exception is a common exception that can occur in many places and even in languages that don\u0026rsquo;t want to use exceptions, like Haskell. You could do controlled handling of zero division, but doing that everywhere is a massive pain. Though having an exception crash the entire program is a fair way of handling errors. Determinism becomes even harder when raising exceptions and specifically catching them is an integral part of the language. Now a function can either return normally, or it can return with an exception that may or may not be caught. If you do catch it then you have two types of returns you can do, one which can come from anywhere deep inside that function call. And sometimes you may not even be aware of certain exceptions that may happen in your code.\nInteresting takes on handling nondeterminism Haskell is a language that tries to avoid nondeterminism as much as possible, and therefore has some interesting ways to handle the above mentioned problems. Global state is easily solved by simply not allowing any global state. You can write code perfectly fine without it. This was proven by Alonzo Church, the creator of Lambda calculus. Lambda calculus is functionally equivalent to the Turing machine that Alan Turing created, of which all modern computers are based on.\nIO is significantly tricker, and originally Haskell was just a black box that you compiled, ran, and then you would be able to look at the return value of the main function once it was done. It had no actual IO support. As time went on they added support for IO in the main function, but everything else had to be deterministic. Finally they added support for IO everywhere, though very restricted to be able to enforce determinism. IO in Haskell is wrapped in a monad (I will not explain what a monad is). This meant that any function that does IO must return an IO monad. Therefore, any function that does IO must eventually return its IO operation back to the main function. Because of Haskell\u0026rsquo;s type system the IO must also be included in the function types. Looking at a function signature you can immediately know if it does any IO or not. This makes you more wary of using IO. (On a side note, Haskell has debugging functions that lets you print values anywhere, but this is strictly for debugging and they live in the debugging module).\nYet again, Haskell is a prime example of how exceptions can be handled. It has the types Maybe and Either. Maybe simply returns Just a value, or Nothing. This would be similar to having a function return a value, or null. With one slight difference. In Haskell you have to handle the type by checking if it is Just or Nothing before being able to use the value inside it. It\u0026rsquo;s impossible to get something like NullPointerException later because you forgot to check for null. Either type is an extension to Maybe in that instead of Nothing it returns something else. This is usually an error message in the form of a string. But it could be any other value, though you still know whether it was the right or wrong return value.\nInterestingly enough, Java has also built upon this concept of making the exceptions explicit and forcing you to handle it. If a function can throw some exception it has to be declared in the type signature. Anywhere that function is called you must either handle that exception or explicitly write that your new function can also throw that exception since it will propagate up the call stack. And regarding the handling of null values Kotlin uses a method similar to Haskell by forcing you to check if it\u0026rsquo;s null or not before you can use the value.\nThe reason I wanted to talk about nondeterminism is because being aware of it is an important step to writing better code. As we concluded in the previous post, deterministic functions are easier to debug. I encourage you to make your functions deterministic whenever possible and keeping in mind the possible ways you might break that property.\n","permalink":"https://casan.se/blog/programming/nondeterministic-functions/","summary":"In a previous post I talked about deterministic functions and some useful properties they have. I recommend you read that one first if you haven\u0026rsquo;t already. But deterministic functions aren\u0026rsquo;t all that useful to get tasks done. They only transform data from one form to another, but cannot actually read input from the console or even print to the console. A program consisting of only deterministic functions will always perform the same calculation, and given that it doesn\u0026rsquo;t have input or output it will only use whatever numbers it was compiled with.","title":"Nondeterministic functions"},{"content":"Recently I got the idea that I wanted to experiment with using X11 directly, instead of some higher-level library where buttons and fields already exist as objects. The goal was to get a better idea of how the graphics and window systems works in Linux (X11 specifically). This wasn\u0026rsquo;t a huge project and I only got to see parts of it before moving on, but it was a very interesting experience. Especially for someone like me who has barely done any kind of graphics programming before. In doing this project I started with a template from this guide that contained setting up a window and listening to user events inside the window. The official documentation was also very useful, along with the manpages in Linux. This post contains some short explanations of problems I encountered and discoveries I made during this project. Github: https://github.com/cappe987/graphics-simulation.\nThe gif above shows my project. It is small blue balls bouncing on green balls. The blue balls fall from top to bottom and green balls can be placed or removed by left and right clicking. I know the bounce physics isn\u0026rsquo;t correct but ignore that for now. The goal was just to learn about X11, not to make great physics.\nHow X11 handles windows and input X11 has 4 important variables.\nDisplay* display; int screen; Window window; GC gc; display is a pointer to the X Server and is passed to almost every function in X11. screen refers to which monitor to use. window is the actual window that you are working with. gc stands for Graphics Context and holds information on how things should be drawn. Information such as colors, fonts, etc. Any changes made to the window requires both the display and the window to be passed to the function. If it should draw something on the window itself then the gc also needs to be passed.\nX11 handles input through events. It waits for events to happen and then it runs. When nothing is happening it is not actively running any code. By default it doesn\u0026rsquo;t listen to any events. You need to set a mask for which events you want to listen to. This is done through a bitmask with the bitwise or-operator | in C.\nHere\u0026rsquo;s an example of setting the events\nXSelectInput(display, window, ExposureMask | ButtonPressMask) This code sets the window to only generate events for exposure (when contents in the window can no longer be seen due to resizing) and button press (mouse buttons). Button events is probably the simplest to understand, it generates an event when you click a button on the mouse, and the event struct received will contain information about which button. Exposure event is useful if you expect the user to resize the window. Because when it is resized it may need to be redrawn if some things in the window are no longer visible.\nFPS-independent speed At the start of the project the speed of the balls were dependent on the FPS of the application. This is often not desirable in games or animations because that means that the higher FPS the faster the game will move and you cannot manually control the speed. FPS in games is often not stable. It doesn\u0026rsquo;t sit on just 60 FPS unless you lock it to max 60 and assuming your computer can handle it, but there\u0026rsquo;s still the possibility that it occasionally drops slightly. So even small fluctuations could change the game speed if you don\u0026rsquo;t lock it. The way I solved it, which probably isn\u0026rsquo;t a perfect solution, was to lock it to a set amount of moves per second. Depending on how much time had passed since the last move it multiplies the velocity vector by the time difference. A time difference of 1 would indicate that exactly the right amount of time had passed. Shorter time would make it move shorter distance and longer time makes it do longer jumps, which evens it out. Although one problem with this would be if you have a really slow computer the time difference will become really high and the blue balls will make large jumps, possibly going over the green balls.\nCircles and Arcs The X11 library provides types for drawing rectangles and arcs (circles). One issue I had with the pre-existing XArc was that it uses integers for position. Which makes sense given that the screen is made up of pixels and that is how X11 interacts with the screen, it colors pixels on it. But I couldn\u0026rsquo;t use this XArc for my animation because of several reasons. One being that I simply needed more fields, like ones for velocity. But I also wanted float for the position. This was so the multiplication with the time difference previously explained would work correctly. Too low velocity and too fast computer could possibly result in the integers always being rounded down and therefore getting stuck in one place. I never experienced this issue myself, but I wanted to counteract this anyways.\nAnother annoying issue with the circles is that when you tell X11 to draw a circle (using XDrawArc) the coordinates you give are in the upper left corner, as if it was a rectangle. It is not at the center of the circle. At the start I made a function called get_center that returned the x and y of the center of the circle. When I later made my own Circle struct I decided to make the coordinates be the center of the circle because those coordinates are used much more when moving and calculating the physics and collisions. Then I simply provided a function for drawing the circles that subtracted the radius from the x and y positions, since this was the only place I would ever need those coordinates. Everywhere else I needed the center of the circle.\nRedrawing and double buffers X11 doesn\u0026rsquo;t have any concept of objects in the window. All it does is draw the pixels you tell it to draw. To move a ball I had to paint over the previous position with white (the background color) and then redraw it in the new position. This meant that I had to erase and redraw everything that was somehow affected by the change in position. The blue balls would sometimes enter the green balls slightly, and since I wasn\u0026rsquo;t redrawing the green balls (as they didn\u0026rsquo;t move) it ended up making tunnels and craters in the green balls when the balls erased their previous position with white. Due to this issue I decided to always draw the green balls for every frame as well.\nThis was working alright for a while, but then I decided to increase the amount of blue balls. This resulted in a lot of flickering on the screen. To solve this I used the Xdbe functions for double buffers. This provides a drawable buffer that you can write to and when you are done writing you call the function XdbeSwapBuffers to place the buffer contents on screen. The buffer also always starts blank so everything has to be redrawn every frame, so I had no choice but to draw the green balls as well. Though the double buffers worked wonderfully. I was able to run it with 10 000 blue balls at once without any graphical glitches, although the window was so cluttered it was hard to see what was happening.\nMultithreading The function XNextEvent locks the thread until it receives an event (an event specified by the previously mentioned XSelectInput). To be able to have continuous updating of the animation while also taking user input I needed to use a second thread. One thread to handle just the input, and another to handle all the animations. This also raises the major issue of multithreading, accessing the same data from different threads. I use the input to place down green balls, but the animation thread also needs the green balls to draw them. The usual solution to this is called mutex. A mutex is a variable that can be locked and unlocked. If it is locked then someone else (another thread) that tries to lock it will have to wait until the first thread unlocks it. Although mutex is no magical solution, you still need to be careful to make sure you lock it in all places where it accesses shared data. Semaphores also exist if you want to allow several, although limited, amount of threads to access something. But in this project I didn\u0026rsquo;t use the mutex library because X11 has its own functions for manipulating the window from different threads. To use these you must first initalize it with XInitThreads(), otherwise it won\u0026rsquo;t even let you access it from other threads. Then you use XLockDisplay(display), which is pretty much the pthread_mutex_lock(mutex) function. I don\u0026rsquo;t know what makes it special, but it was easy to use this to lock some shared variables as well since most of the time when I needed to lock something I was also gonna draw.\n","permalink":"https://casan.se/blog/programming/bouncing-balls-simulation-using-x11/","summary":"Recently I got the idea that I wanted to experiment with using X11 directly, instead of some higher-level library where buttons and fields already exist as objects. The goal was to get a better idea of how the graphics and window systems works in Linux (X11 specifically). This wasn\u0026rsquo;t a huge project and I only got to see parts of it before moving on, but it was a very interesting experience.","title":"Bouncing balls simulation using X11"},{"content":" In this post we will take a closer look at functions and some mathematical concepts relating to them. This post assumes you are familiar working with functions in some programming language and some mathematical knowledge of functions as well (like knowing what \\(y = f(x)\\) means and what a set is).\nTerminology and notation Before getting into the main topic I want to cover some terminology and notation. Sets are denoted as an uppercase letter in math-style font like \\(A\\). When I say that an element maps to another it is simply a way of saying that some element \\(x\\), when given as argument to \\(f\\) will return an element \\(y\\), or as an equation \\(y = f(x)\\), \\(x\\) maps to \\(y\\). A transformation from \\(x\\) to \\(y\\). A function/mapping is simply a transformation from one thing to another. One goes in, another goes out.\nIt is also good to know the mathematical notation for the types of a function. \\(f : A \\rightarrow B\\) represents a function \\(f\\) that takes a single argument of type \\(A\\) and returns a value of type \\(B\\). To be more accurate, \\(A\\) and \\(B\\) aren\u0026rsquo;t types, but rather a set of possible values that can be input/output. But in the world of programming we often represent them as types like int, string, float, or similar. For example, the \\(log\\) function could be written as \\(log : \\mathbb{R}^+ \\rightarrow \\mathbb{R}\\), which means all positive real numbers as input and all real numbers as output. Although in the programming world it would instead be written as \\(log : \\text{float} \\rightarrow \\text{float}\\). \\(A\\) is called the domain. \\(B\\) is called the codomain. If we want to be more exact we use the term range to indicate all possible values that \\(f\\) can return. Which means that the range is a subset of the codomain. For example, for the function \\(f(x) = x^2\\) we can say it has the codomain \\(\\mathbb{R}\\), but if you inspect the function you will see that the range is only \\(\\mathbb{R}^+\\), only the positive real numbers since the function can never return any negative numbers.\nDeterminism and the definition of a function Going strictly by the mathematical definition we don\u0026rsquo;t have any reference types (pointers) and no void functions. A function must always return a value for any element that is in its domain. Exceptions like values that will cause division by zero or other undefined computations will not be in that domain and are expected to not be put into the function. To be able to make any useful arguments regarding a function we must also require it to be pure.\nThe concept of pure functions only exist in the programming world, because in the world of math every function is pure. A pure function is one where the the return value only depends on the arguments. There cannot be any side effects. The function cannot print or write any values to external memory, and cannot read any values from the outside. These are what we call side-effects, inputs/outputs that are not the function arguments and return value. If we allow side-effects then the function becomes inherently non-deterministic, unpredictable. From here on when I say input I mean the arguments and output means return value. The simplest pure function we can have is the \u0026ldquo;constant function\u0026rdquo;. It takes 0 arguments and returns the same value every time it is called. Since we can\u0026rsquo;t have any side-effects that is all a function with no input can do. Below is an example of a constant function (written in Python). As you can see, if we don\u0026rsquo;t have any arguments there is no data to work with if we aren\u0026rsquo;t allowed side-effects.\ndef pi(): return 3.14159265359 This may seem restricting, but determinism in itself is a very useful property for a function. The same input will always give the same output. It\u0026rsquo;s very easy to test as it doesn\u0026rsquo;t depend on any state, which also makes it easier to debug. If you have some logical mistake in a deterministic function then all you need to do is find the mistake inside that function (assuming the input is correct, otherwise the issue is elsewhere). If it is non-deterministic function then you may need to look at all state that it is working with to find what goes wrong. You may need to debug much more outside of the function in question to find what leads to the invalid state.\nIf you don\u0026rsquo;t care about the formal definition then just skip to the next section. A formal definition is that a function \\(f : A \\rightarrow B\\) maps every element in a set \\(A\\) to some element in a set \\(B\\), and each element in \\(A\\) can only map to a single element in \\(B\\). This means that for any value \\(x \\in A\\), \\(f(x)\\) must always be the same value. Or expressed as a logical statement below: if \\(f(x)\\) returns two values \\(y_1\\) and \\(y_2\\) from the same \\(x\\) then \\(y_1 = y_2\\).\n\\( \\forall x. x \\in A. y_1 = f(x) \\wedge y_2 = f(x) \\implies y_1 = y_2 \\)\nIf the return values were different then that would contradict the definition of a function and it would also break the purity and determinism. If \\(y_1\\) and \\(y_2\\) are different for the same \\(x\\) then the function obviously has side-effects.\nInjective, surjective, and bijective An injective function is a function \\(f : A \\rightarrow B\\) where no two elements in \\(A\\) returns the same element in \\(B\\). This is a function that is reversible because each value in the range has only one corresponding value in the domain. But that is only true for the range. Every value \\(y = f(x)\\) can be reversed to get back \\(x\\), but not every value in the codomain. The codomain may be a superset of the range, so there may be elements in the codomain that do not have any corresponding element in the domain. Some properties that derive from this is that the sets of the domain and range must be the same size, and the codomain must be equal or greater than the domain.\nA surjective function is the opposite of an injective. All elements in the codomain must be return values of some input to \\(f\\). Which means that the range = codomain. But the domain can be larger than the codomain since multiple elements of \\(A\\) can map to the same element of \\(B\\). There is no guarantee that the elements in \\(B\\) can be reversed since there may be several values of \\(A\\) that can give that output.\nBijective functions are functions that are both injective and surjective. When we put those two together we get a function that is a one-to-one mapping between the domain and codomain. It is always reversible in both directions and you can always create an inverse function that gives back the original (usually denoted \\(f^{-1}\\), where \\(f^{-1}(f(x)) = x\\)). Inverse operations can be useful when you want to undo something you have previously done or when you need to find out what the original was. For example, addition and subtraction are inverses. If we have \\(f(x) = x + 5\\) then the inverse of that is \\(f^{-1}(x) = x - 5\\).\nWhile bijective functions have a clear use-case of inverse operations, injective and surjective functions may be harder to find a good use for. Although it can be something to keep in mind when programming as they have some unique properties which you may be able to use for something.\nChaining functions When we have functions that always return values and have well-defined domains and codomains then we can easily chain them. This is called function composition. In mathematical notation it looks like this \\( g \\circ f\\), which simply means \\(g(f(x))\\). But to do this the functions must have matching domains and codomains. If \\(f : A \\rightarrow B\\) then we must have \\(g : B \\rightarrow C\\) to be able to chain them. So if \\(h = g \\circ f\\) then \\(h : A \\rightarrow C\\) since it performs both \\(f\\) and \\(g\\). Composing functions like this is very good for creating a long pipeline of transformations. And if we stick to using deterministic functions then it becomes very clean looking code and the different components get separated into distinct parts where one returns what the next needs. When chaining functions it is important to keep track on the domain and range of your functions. If \\(g\\) can return a value that \\(f\\) cannot take as input then you have a problem.\nThe concept of chaining can be applied on reference types as well. It requires every method to return self or this or whatever your language calls the reference an object has to itself. So you could call methods like x.f().g().h() and so on, where each method modifies the internal structure as if those were the arguments. This is commonly seen in the builder pattern (C# example below). The non-object-oriented equivalent of that is h(g(f(x))), where x could just as well just be an object that holds some variables. The problem with determinism and purity in object oriented programming comes when a method both modifies the internal state and returns a separate value. Because at that point it has changed some state that is separate from the return value.\nHumanBuilder builder = new HumanBuilder(); Human human = builder.AddHead().AddBody().AddArms().AddLegs().Build(); There is nothing wrong with non-deterministic functions. Non-determinism is required to make any interesting program. And a lot of object oriented programming revolves around changing state. It just takes a different approach than the mathematical side. Although I do encourage you to write as much deterministic code as possible.\nOther types of mappings Dictionaries, HashMaps, HashDictionaries, and the like are all mappings from one set to another. The key is the input and the value is the output. And as long as you don\u0026rsquo;t modify them they are deterministic. Although modifying them are often a key feature of using them. They may not be considered functions, but they are definitely relations, a superset of functions. A relation is just a mapping from one element to another, often denoted (1,2) when 1 maps to 2 (in function notation: \\(2 = f(1)\\)). Although relations are often represented as whole sets of mappings to indicate all possible relations like \\({(1,2), (2,3), (3,4), (4,5)}\\) to indicate a subset of the function \\(f(x) = x + 1\\) for integers.\nSwitch statements are also a type of mapping, and a function, from some variable value to a block of code. If that block of code is just a function that returns its value then you have a mapping from a value to a function (C# example below).\nswitch (x){ case 1 : return f(); case 2 : return g(); default: return h(); } Everything stated here extends easily to functions with multiple arguments or multiple return values, usually denoted as tuples. Although deciding if such functions are injective or surjective may become much harder. The notation for such a function is \\(f : A \\times B \\rightarrow C \\times D\\), where \\(A \\times B\\) means the Cartesian product of the sets \\(A\\) and \\(B\\). Meaning all possible combinations of the elements of the two sets. When you have multiple arguments the domain increases a whole lot in size, since now every element in \\(A\\) can be paired with every element in \\(B\\). The total domain size becomes \\(\\vert A \\vert \\times \\vert B \\vert\\) (size of \\(A\\) multiplied by size of \\(B\\)).\nFunctions and relations are everywhere in programming. In places you never considered before. Even your browser is a mapping that takes a URL and returns a webpage. Keep an eye out for them and see if you can use that knowledge to write cleaner and better code, and maybe catch some bugs.\n","permalink":"https://casan.se/blog/math/functions-a-deep-dive/","summary":"In this post we will take a closer look at functions and some mathematical concepts relating to them. This post assumes you are familiar working with functions in some programming language and some mathematical knowledge of functions as well (like knowing what \\(y = f(x)\\) means and what a set is).\nTerminology and notation Before getting into the main topic I want to cover some terminology and notation. Sets are denoted as an uppercase letter in math-style font like \\(A\\).","title":"Functions: a deep dive"},{"content":"Cellular automata is way to model biological systems through a set of cells that change their state based on their surroundings [1]. It requires some form of grid to operate on, where each location on the grid contains one cell. The cells are based on integer coordinates on the grid. The second requirement is a set of rules that determine when a cell should change state and to what state. The third and final requirement is an initial state, what it looks like at the start. Then the rules can be applied on all the cells to update their states. Applying the rules can be done as many times as you wish. Each application is called a step. Over time, when applying these rules certain behaviours may exhibit in the grid. This shows the interaction of the cells as a system.\nOne of the most famous cellular automata is Conway\u0026rsquo;s Game of Life and it models over- and underpopulation [1]. It takes place on a 2-dimensional plane of integers. Each cell can either be alive or dead. The rules are as follows:\nIf a cell has 0 or 1 living neighbour (out of its 8 neighbours) then it dies. If it is alive and has 2 or 3 living neighbours then it stays alive. If it is dead and has 3 living neighbours then it becomes alive. If it has 4 or more living neighbours then it dies. Too few neighbours and it dies of underpopulation, too many and it dies of overpopulation. Just the right amount and new cells are born. The image above shows a glider in Conway\u0026rsquo;s Game of Life. It is a set of cells that will continuously move diagonally (towards the bottom right in this image) until they collide with something else that disrupts the formation. Gliders are a stable formation because over a couple of steps it will return to its initial formation, thus making it go on forever in the same pattern.\nSome useful applications of cellular automata [2] include simulation of:\nGas spread Forest fire spread Bacterial growth Flow of electricity These are areas that can be hard and complex to model accurately using conventional computation methods, but by constructing simple rules a cellular automaton can represent them very well, and be computed in a very efficient manner.\nHeinonen and Pukkala [3] have applied cellular automata to forest planning using two-state cells on a hexagonal grid. With two states, one represented a stand (area of trees planted at the same time) that should be thinned out by cutting down some trees and the other represented a stand that should be kept as is. They later added a third state that indicated regeneration cutting, which means cutting down a lot or most of the trees to prepare for new trees. Using the cellular automata they have to compute way less than previous methods and also managed to get better results.\nCellular automata can also be self-organizing [4]. This means that over time they will tend towards a certain pattern, much like certain constructs that can be found in nature. Like cellular automata, biological systems also use small components to gradually build structures.\nReferences [1] Eric W Weisstein. Cellular Automaton. URL: https://mathworld.wolfram.com/CellularAutomaton.html\n[2] Michael J Young. Typical Uses of Cellular Automata. Nov. 2006. URL: http://www.mjyonline.com/CellularAutomataUses.htm\n[3] Tero Heinonen and Timo Pukkala. \u0026ldquo;The use of cellular automaton approach in forest planning\u0026rdquo;. In: Canadian Journal of Forest Research 37 (Nov. 2007), pp. 2188-2200. DOI: 10.1139/X07-073\n[4] Stephen Wolfram. \u0026ldquo;Cellular Automata\u0026rdquo;. In: (1983)\n","permalink":"https://casan.se/blog/computer_science/cellular-automata/","summary":"Cellular automata is way to model biological systems through a set of cells that change their state based on their surroundings [1]. It requires some form of grid to operate on, where each location on the grid contains one cell. The cells are based on integer coordinates on the grid. The second requirement is a set of rules that determine when a cell should change state and to what state. The third and final requirement is an initial state, what it looks like at the start.","title":"Cellular automata"},{"content":"This article is intended to be read by someone who is looking to learn programming or is just starting out. This doesn\u0026rsquo;t tell you what programming language to choose or what tutorials to follow. Only some general tips to keep in mind when learning.\nWhat is programming? Many people think that programming is all about writing code. But the main job a programmer does is solve problems. Writing code is just the actual solution to the problem. You still need to figure out how to solve it first. Just like an engineer must plan and measure everything before building a bridge, the bridge itself is the final product but many hours go into just figuring out how it should be done. Or if we compare it to math. Solving a math problem requires you to first figure out what kind of problem it is. What formulas will you need? Then once you know that you can start doing the calculations, the equivalent of writing code.\nProgramming problems are often generalized problems. The code should work for any values of x and y. A basic math question could be “If Bob has $5 and John has $3. Who has more money?”. This question can easily be answered by looking at the values. The bigger number is the answer. But if we generalize this to “If Bob has $x and John has $y. Who has more money?”. Now we only have x and y to work with, we can’t simply look at the number. We need to formalize a way to decide who has more money in terms of the variables x and y. Given that the answer depends on the values of x and y we need to ask the computer a question. The simplest question we can ask is “Is x bigger than y?”. If the answer is yes, then Bob has more. Else, John has more (or equal).\nHow to learn programming The best way, and often the only way, to learn something properly is learning by doing. You don’t get good at math by reading problems, you get good at it by doing exercises, by using math. Same is true for programming. It is very important that you write a lot of code by yourself. Following along with a tutorial is a good way to learn, but don’t just write the same code they do. Play around with their code. Try your own variations. Think for yourself!\nI don’t know any French, but I can still write down what someone else is writing without ever learning what it means or how to use the words I’ve written. That’s why it’s important to experiment by yourself. And unlike learning a spoken language, programming languages will tell you if you are doing right or wrong. If it’s a syntax error, which means invalid grammar in the programming world, then the program will tell you so and approximately where the problem is (it\u0026rsquo;s not always exact, if you can\u0026rsquo;t find it then try looking at the lines before the error. Sometimes mistakes on previous lines aren\u0026rsquo;t considered errors until something else comes later). If it’s a logical error, you will find that out when you run the code and it doesn’t work as expected.\nWhich brings me to another point. Don’t be afraid of running your code. I often see beginners asking “is this correct?” and sending a code snippet. I always respond with “does it give the expected result?”, which more often than not gives back the response “I don’t know, I haven’t tried it”. Well, why haven’t you tried it? Don’t waste my time asking if it is correct when you can get the right answer in a few seconds yourself. A much better question to ask is “How can I improve this code?” or “Can I have some feedback on my code?” after you have solved a problem.\nHow to watch/read tutorials When watching tutorials, or reading a book, or whatever way you choose to learn, don’t rush through the topics. Going through 20 topics in one day is going to leave you more confused than when you started. You are overloading your brain with new information. Take one or two topics/videos at a time and spend some time digesting that content until you have a decent understanding of it. I sometimes see these several hours long tutorials show up on YouTube. Just because it\u0026rsquo;s one video doesn\u0026rsquo;t mean you should binge watch it and then expect to have learnt it all. Most of them are divided into clear sections. Split up those 4-8 hours of video over the course of 2 weeks. Watch 30 minutes daily and focus on those parts. As I mentioned earlier, play around with the concepts covered in the tutorial. If you don’t understand it, rewatch it and try again, or ask someone else for an explanation. Sometimes it helps to hear it explained in a different way.\nIf you are struggling to understand or to solve a problem:\nGo back and read again to make sure you understand the concepts needed. Google for how to solve the problem (see section below on How to Google). Ask for help (see section below on How to ask for help) How to Google When Googling for help you should be as specific as possible with your search query. If you are making a snake game, don’t google “How to make a Snake game?”. Instead search about exactly the part you are stuck on. Maybe the question should be “How do I make a program with graphics/colors?” or “How do I make my Snake longer?”, or even better if you can explain it in programming terms. The more general you can make your question the more likely you are to find an answer. This does not mean to ask vague questions. It should still be specific about the problem. It is also a good idea to start your query with the name of the programming language, for example, \u0026ldquo;python snake game how to make snake longer\u0026rdquo;. It can also be a good idea to leave out irrelevant words such as \u0026ldquo;the\u0026rdquo;, \u0026ldquo;i\u0026rdquo;, \u0026ldquo;a\u0026rdquo;. Shorter search queries are often better, as long as you don\u0026rsquo;t leave out any important details.\nHow to ask for help Don’t ask for help immediately. Always try to find an answer online first by Googling. Spend maybe 30 minutes on Google, reading relevant articles or watching videos. Maybe even sleep on it and try again tomorrow. After that you can ask for help online. As an active helper online it is not fun to help someone who has put in zero effort to understand their problem. Sometimes I see people just posting the code and saying \u0026ldquo;it doesn\u0026rsquo;t work\u0026rdquo;. Sometimes they can\u0026rsquo;t explain in what way their code doesn\u0026rsquo;t work, that shows that you haven\u0026rsquo;t even understood what you are trying to do. It should be easy to say \u0026ldquo;X should happen but Y happens instead\u0026rdquo;. Why should we put in the effort to help you if you aren\u0026rsquo;t willing to put in the effort yourself? By searching online first you may also get some understanding, which might get clarified by someone explaining directly. Even if you don\u0026rsquo;t find your answer online you will still have had some ideas put in the back of your mind.\nDiscord communities are a great place for beginners. When you finally ask for help, be as specific as possible. Explain the issue in detail and provide all information about the problem (if it’s an exercise you have found somewhere then please send the whole exercise description or a link to it). Sending parts of the relevant code is also very helpful. Going back to the previous example of “How do I make my snake longer?” you will probably need to explain how you have coded your snake so far and show the code for it. That way you can get help with how you should proceed. Without knowing what your code looks like you often can’t get an exact answer to such a question. Depending on what your code looks like one solution may be much more difficult than another.\nI also recommend reading this: dont\u0026rsquo; ask to ask, just ask. In short, don\u0026rsquo;t ask \u0026ldquo;can anyone help me with X?\u0026rdquo;. Instead explain your problem directly.\n","permalink":"https://casan.se/blog/programming/programming-how-to-learn-it-and-how-to-get-help./","summary":"This article is intended to be read by someone who is looking to learn programming or is just starting out. This doesn\u0026rsquo;t tell you what programming language to choose or what tutorials to follow. Only some general tips to keep in mind when learning.\nWhat is programming? Many people think that programming is all about writing code. But the main job a programmer does is solve problems. Writing code is just the actual solution to the problem.","title":"Programming: how to learn it and how to get help."},{"content":"Introduction The W algorithm developed by J. Hindley and R. Milner is used to infer the types of a programming language where no types have been explicitly written. Several functional languages today use this kind of inference, for exampel F# and Haskell (although Haskellers still like to write out the types). The method works by assuming everything is a different generic type. The types of any pre-existing functions and any constants will be known before starting. So if there is a 0 in the code then that is an integer, 0.0 is a float, [] is a generic list.\nFinding the types Types with an apostrophe before them means they are generic. A type is written with the syntax variable_name : variable_type.\nWe begin with a simple example. Below we have the identity function written in F#. It takes in an element and returns back the same element.\nlet id x = x To calculate the types of this function we start by setting all unknown values to a generic type.\nx : \u0026#39;a id : \u0026#39;b -\u0026gt; \u0026#39;c The variable x has the generic type 'a and the function id has the generic type of 'b -\u0026gt; 'c. Because the input of the function is x and x has the generic type 'a then the function input must also have the same type.\nx : \u0026#39;a id : \u0026#39;a -\u0026gt; \u0026#39;c The only possible output of this function is also the value of x. Thus the return value of the function must also be 'a.\nx : \u0026#39;a id : \u0026#39;a -\u0026gt; \u0026#39;a And now we\u0026rsquo;re done with that function. All values have been reduced to the same type. There are no further reductions to be done. The identity function has the type 'a -\u0026gt; 'a because it will always return the same value it is given.\nA more complicated example Below we have a function of which we do not know any of the types for.\nlet rec myfunc f xs = match xs with | [] -\u0026gt; [] | x::xs -\u0026gt; f x :: myfunc f xs We start by finding the types of all our constants and pre-existing functions and then giving a generic type to all our unknowns.\n[] : \u0026#39;a list (::) : \u0026#39;b -\u0026gt; \u0026#39;b list -\u0026gt; \u0026#39;b list f : \u0026#39;c x : \u0026#39;d xs : \u0026#39;e myfunc : \u0026#39;f We can see that the parameters for myfunc are f and xs. Therefore 'f = 'c -\u0026gt; 'e -\u0026gt; 'g\nf : \u0026#39;c x : \u0026#39;d xs : \u0026#39;e myfunc : \u0026#39;c -\u0026gt; \u0026#39;e -\u0026gt; \u0026#39;g To start figuring out what the types are we can start by looking at the match statement. It matches on xs and the first pattern it suggests that it is of type 'a list. The second pattern is a deconstructing pattern with the cons operator. Because of the :: operator that means x : 'b and xs : 'b list, and the return value of the :: operation and the second pattern match is therefore 'b list. We can therefore say that 'e = 'a list = 'b list, which in turn implies that 'd = 'a = 'b.\nf : \u0026#39;c x : \u0026#39;a xs : \u0026#39;a list myfunc : \u0026#39;c -\u0026gt; \u0026#39;a list -\u0026gt; \u0026#39;g Now there\u0026rsquo;s only 'c and 'g left to figure out. We\u0026rsquo;ll start with 'g'. The return value of the first pattern match is an empty list 'h list. Note that this is the second empty list we use and it is not guaranteed to be the same type as the first. Same goes for the :: operator. The second pattern match returns the result of the :: operation, which we\u0026rsquo;ll call 'i list. If the cons operator is to work on its arguments the left side must be 'i and the right side must be 'i list. Therefore myfunc must return an 'i list. And since the only other return value of myfunc is 'h list we can draw the conclusion that 'g = 'h list = 'i list, which implies 'h = 'i. Since we said earlier that x : 'a and the left input to :: is 'h we know that f : 'a -\u0026gt; 'h.\nf : \u0026#39;a -\u0026gt; \u0026#39;h x : \u0026#39;a xs : \u0026#39;a list myfunc : (\u0026#39;a -\u0026gt; \u0026#39;h) -\u0026gt; \u0026#39;a list -\u0026gt; \u0026#39;h list And then we fix it up a little by using only the first letters of the alphabet.\nf : \u0026#39;a -\u0026gt; \u0026#39;b x : \u0026#39;a xs : \u0026#39;a list myfunc : (\u0026#39;a -\u0026gt; \u0026#39;b) -\u0026gt; \u0026#39;a list -\u0026gt; \u0026#39;b list The final type of myfunc is ('a -\u0026gt; 'b) -\u0026gt; 'a list -\u0026gt; 'b list. Any experienced functional programmer should now see what function it is, if they didn\u0026rsquo;t already see it the second they saw the function definition. It is of course the famous map function, or more specifically the map function for lists.\nIntroducing a type error Now lets say we instead have the same function but a :: switched out for a +.\nlet rec myfunc f xs = match xs with | [] -\u0026gt; [] | x::xs -\u0026gt; f x + myfunc f xs The plus operator has the type (+) : 'num -\u0026gt; 'num -\u0026gt; 'num where 'num symbolizes a generic number type (eg. int, float, double). This would mean that f : 'a -\u0026gt; 'num. But the returning value of myfunc can\u0026rsquo;t be a 'num because the first case of the pattern match says it\u0026rsquo;s an 'h list. Now we have reached a type error and compilation can stop here. This code will not compile because the types for + doesn\u0026rsquo;t match.\nExercise for the reader let exercise l = match l with | [] -\u0026gt; 0 | x::xs -\u0026gt; if x \u0026gt; 0 then x + exercise xs else exercise xs ","permalink":"https://casan.se/blog/computer_science/type-inference-with-hindley-milner-w-algorithm/","summary":"Introduction The W algorithm developed by J. Hindley and R. Milner is used to infer the types of a programming language where no types have been explicitly written. Several functional languages today use this kind of inference, for exampel F# and Haskell (although Haskellers still like to write out the types). The method works by assuming everything is a different generic type. The types of any pre-existing functions and any constants will be known before starting.","title":"Type inference with Hindley-Milner W algorithm"},{"content":"Introduction Most programming languages today have a bunch of operators for different purposes. Usually for mathematical, logical, comparison, and bitwise operations. +, -, *, /, \u0026amp;\u0026amp;, ||, exists in probably every modern language. Developers expect them to exist; no one would want to program without them.\nMany languages like to extend their operators, usually +, to work on several types. Python allows using the plus operator on both strings and numbers. The interpreter accepts both \u0026quot;AB\u0026quot; + \u0026quot;CD\u0026quot; and 3 + 5; but these are not the same operation, just the same operator. Addition on numbers is commutative, 3 + 5 == 5 + 3; addition on strings is not, \u0026quot;A\u0026quot; + \u0026quot;B\u0026quot; != \u0026quot;B\u0026quot; + \u0026quot;A\u0026quot;.\nOperator:\na symbol or function denoting an operation (e.g. ×, +).\nOperation:\nan action to be performed on some data.\nLanguages do this through operator overloading, either built in to the compiler/interpreter, or as a part of the actual language. You specify what operation an operator should do for a specific class/type. While integer addition adds up the numbers, string addition (also known as concatenation) appends the second string to the end of the first to create a new string. If you do add any yourself, you shouldn\u0026rsquo;t give them completely different behavior than what the symbol usually means. Using the + operator on Vectors shouldn\u0026rsquo;t perform a cross product just because you thought it seemed convenient.\nOperators as functions You may think of operators as these magical symbols that the developers programmed into the language. While partially true, you can also see an operator as a infix function (excluding Lisp, where everything is prefix). Think of \u0026gt; as a function. What parameters does it have and what does it return?\nClick to expand: greater-than type signature {% highlight fsharp %} (int -\u003e int -\u003e bool) {% endhighlight %} In a more familiar style: {% highlight csharp %} bool greaterThan(int a, int b); {% endhighlight %} This is not accounting for any other possible operator overloading, such as strings or other number types (eg. float/double). Some programming languages lets you treat operators like a function. In the code snippet below, we assign the plus operation to the function addition\naddition = (+) -- (int -\u0026gt; int -\u0026gt; int) addition 3 5 -- returns 8 Haskell also lets you treat regular functions as infix function.\n3 `addition` 5 -- returns 8 You can usually express operators in terms of other operators. In C you can define + and - using bitwise operators; and in turn you can define * and / with those. Assuming you have other language constructs such as loops and if-statements available. Integer equality and comparison can be expressed with the help of subtraction. Logical operators can be expressed with equality.\nAt the time of writing this I am currently working on making an interpreter for my own Lisp-like language. I was having some trouble figuring out how to handle the operators. Do I treat them as functions? Or is it better to see them as operators and parse them as such? To minimize the amount of operators I realized I could define the logical operators with the help of only the equality operator; and could thus put them in the standard library for the language. A few less operators to worry about. Unfortunately I couldn\u0026rsquo;t take it any further than that; as the language I\u0026rsquo;m making is rather high-level. The rest of the standard operators are hardcoded in the parser, but are parsed as function calls to allow for easier partial application and less types to worry about.\nbool and(bool a, bool b){ if(a == true){ if(b == true){ return true; } } return false; } Defining your own operators Once again, only some languages allow defining custom operators. When talking about operators I am speficially talking about binary operators in this context. An example of a unary operator (one argument) is - when placed right in front of a numerical to create a negative number a = -2. A ternary operator (three arugments) commonly exists in the form of a one line if-else statement condition ? when true : when false. Less common operators like unary and ternary have their uses, but not in this post.\nDefining your own operators has no logical benefit in the sense that it changes the way your program works. You could consider it syntactic sugar, but it can improve readability a lot. If you find yourself in a project where you are using a certain functionality a lot, you can consider if you want to make a custom operator. The symbols I chose for the example below has no meaning in any context that I am aware of, other than that they are legal symbols for custom operators in Haskell.\nadd5div a b = (a + 5) / (b + 5) ( \u0026lt;@\u0026gt; ) a b = add5div a b 10 \u0026lt;@\u0026gt; 5 -- returns 1.5 Haskell has a bunch of code-defined operators in the standard libary to help with certain actions in its very intricate type system. For example: $, \u0026lt;$\u0026gt;, \u0026lt;*\u0026gt;, and \u0026gt;\u0026gt;=.\nDefining custom operators is more common in functional programming languages than imperative ones. Imperative has all these state-manipulating statements and function calls, while in functional languages everything is done by composing functions, which makes operators very useful. Instead of passing two different data into a function as arguments, you use that function as an operator to make it look more elegant. Function calls become less cluttered and, if you are familiar with the operator, easier to read.\n","permalink":"https://casan.se/blog/programming/operators-are-functions-too/","summary":"Introduction Most programming languages today have a bunch of operators for different purposes. Usually for mathematical, logical, comparison, and bitwise operations. +, -, *, /, \u0026amp;\u0026amp;, ||, exists in probably every modern language. Developers expect them to exist; no one would want to program without them.\nMany languages like to extend their operators, usually +, to work on several types. Python allows using the plus operator on both strings and numbers.","title":"Operators are functions too"},{"content":"Static and dynamic typing explained Static typing can find type errors reliably at compile time, which should increase the reliability of the delivered program -- Wikipedia\nA statically typed programming language checks that the types of everything matches. If something doesn\u0026rsquo;t match, the compiler stops and displays an error. This guarantees type safety to a certain degree, without even running the program. A variable has one type and one type only. It can never change its type. This eliminates a lot of errors, and you can spend less time testing it.\nA language with dynamic type checking has its types checked during runtime. These languages often leave out type declarations in the code, leading to less code to read and write. Checking during runtime adds extra overhead when running, and increases the likelihood of runtime type error. Since it only checks the types it encounters, it won\u0026rsquo;t validate the types in any execution paths you don\u0026rsquo;t test.\nThe code snippet below contains two separate execution paths, and depending on what value some_condition holds, the else condition may or may not run.\nif some_condition: print(1 + 1) else: print(1 + \u0026#34;1\u0026#34;) Someone who has used Python may see that the addition in the else path will throw a type error. A statically typed language would catch the error compile-time. Python, however, has a dynamic type system and will not notice the type error if execution doesn\u0026rsquo;t go down that path. Imagine the condition being something that evaluates to True 99% of the time. You could continue coding for a good while before noticing your mistake.\nStatically typed languages, while type safe to a certain degree, can still contain dynamically typed parts. Languages that make use of inheritance (eg. C#, Java, C++) tend to mix static and dynamic typing. A variable of type A can also hold an instance of any subclass to A. This works because any subclass of A can do everything that A can do. Although it can\u0026rsquo;t use the subclass\u0026rsquo; additional functionalities while assigned to a variable of the supertype, you can downcast the object to its actual type.\nWhen a variable contains an object of a different type than declared, we speak of actual type (the type of the object, possibly a subclass) vs. apparent type (the type of the variable). Here the dynamic part comes in. Because the actual type of the object may differ from the apparent type, we can\u0026rsquo;t know at compile-time if we can downcast. Different execution paths may lead to the variable containing a different subclass.\nOnly when we know the actual type of the variable can we tell if the code allows the downcasting or not. If not it usually results in a runtime error.\nStatic typing Statically typed languages tend to have better code completion. With your text editor correctly set up you can get red squiggly lines under a lot of what would result in a compiler error, saving you some time. You can also see the type signatures of variables and functions, reducing the chances of making incorrect assumptions.\nmap(function, iterables) The snippet above shows the type signature for the function map in Python. We can see that it wants a function and an iterable for input, according to the hopefully well-named parameters. Without knowing the meaning of map you can\u0026rsquo;t tell what arguments you should pass to it. You could somewhat assume what counts as iterable, but it doesn\u0026rsquo;t classify exactly what defines an iterable. Looking at the function parameter we can\u0026rsquo;t tell anything about what type the argument should have.\nIEnumerable\u0026lt;R\u0026gt; Select\u0026lt;T,R\u0026gt;(IEnumerable\u0026lt;T\u0026gt;, Func\u0026lt;T,R\u0026gt;) Here we have the type signature for the same function in C#. T and R represents two generic types. We do not care about the actual types of the arguments, as long as all T\u0026rsquo;s have the same type; and same goes for the R\u0026rsquo;s.\nFor the first argument you can pass it any C# class that implements the interface IEnumerable\u0026lt;T\u0026gt; of any type T. As opposed to Python, it clearly states what defines an iterable.\nFunc\u0026lt;T,R\u0026gt; represents a function that takes a value of type T and returns a value of type R. This means that if we pass in a list of integers, we must also pass in a function that takes an integer as input, and we will receive an IEnumerable of type R as output. The compiler will complain if we pass it anything that doesn\u0026rsquo;t match.\nStatic typing tends to make languages more verbose when explicitly stating the types, but not necessarily. Some languages have a strong type inference.\nmap f [] = [] map f (x:xs) = (f x) : map f xs The snippet above shows a simple implementation of map in Haskell. By analyzing how I\u0026rsquo;ve used lists, list operators, and functions, it can infer the type signature (a -\u0026gt; b) -\u0026gt; [a] -\u0026gt; [b]. This matches the type signature of C#\u0026rsquo;s Select, a is the T and b is the R, although the parameters have swapped places. C# has it the other way because there you typically use it by doing someObject.Select(function) on an object, instead of passing in the object as a parameter. Haskell on the other hand, makes use of partial application where it makes more sense to apply the function first.\nDynamic typing Dynamically typed languages, while more overhead, usually have no compilation stage, and can thus run immediately. Compiling a large project can take several seconds, if not minutes. When you first learn a compiled language, the compilation step may not feel like any hindrance; as a small program can take less than a second to compile.\nNo compilation opens up for a new cool possibility for testing your code. A compiled language can only run your test cases as often as you compile; but in a dynamically typed language you can set up your tests to run whenever you save a file. This way you can constantly keep track of if you break something. If, however, you are a save-maniac like me, who hits Ctrl+S every few seconds, you may want to learn to control yourself.\nDynamic typing tends to lead to less verbose languages. Without static type checking, the need to write out types disappears. This lowers the bar for newcomers. A less verbose language with no strict compiler yelling at them usually appears more welcoming. If you compare the previously mentioned code snippets of map in Python to the Select in C#, you can easily guess which one a new programmer would go for.\nMy experiences and opinions I was first introduced to programming through my university when I began studying computer science. The first course taught the statically typed language C, and later on C#. The whole first year consisted of only statically typed languages before finally reaching the dynamically typed JavaScript in a course in web development. Using JavaScript felt odd, there were a lot of features I missed. Mainly the code completion and variable/function suggestions. I ran into a great deal of bugs and unexpected behavior; I guess some of these can be attributed to JavaScript specifically.\nI have tried Python as well, and I had similar experiences to JavaScript with it. So as of now I am on the static side. I am, however, in no way saying that either is better or worse than the other. This just happens to be my opinion on it. Someone who was taught Python as their first language may be more inclined to prefer dynamic type systems.\n","permalink":"https://casan.se/blog/programming/static-vs-dynamic-typing/","summary":"Static and dynamic typing explained Static typing can find type errors reliably at compile time, which should increase the reliability of the delivered program -- Wikipedia\nA statically typed programming language checks that the types of everything matches. If something doesn\u0026rsquo;t match, the compiler stops and displays an error. This guarantees type safety to a certain degree, without even running the program. A variable has one type and one type only.","title":"Static vs dynamic typing"},{"content":"My name is Casper Andersson and I currently work as a software developer. I enjoy most types of non-UI programming. I am absolutely trash at doing pretty user interfaces and prefer my programs to stay within the terminal. Due to my job my interests currently lie in C programming and computer networks. On top of programming I also really like to write technical blogposts and documentation.\nExperience Working with embedded Linux systems on network switches. Driver development in the Linux kernel, including some upstreaming. Focusing on time synchronization and QoS.\nThesis: Reservoir Computing Approach For Network Intrusion Detection On top of my studies I also worked as an assistant in a course on Introduction to Programming in C a couple times. The tasks included helping students with their assignments and answer any questions they had regarding programming and the course. Notable courses: DVA234 - Databases DVA218 - Data Communication DVA229 - Functional Programming with F# DVA315 - Operating Systems DVA339 - Compiler Theory DVA340 - Artificial Intelligence MAA507 - Mathematics of Internet MMA500 - Discrete Mathematics, a Second Course ","permalink":"https://casan.se/about/","summary":"My name is Casper Andersson and I currently work as a software developer. I enjoy most types of non-UI programming. I am absolutely trash at doing pretty user interfaces and prefer my programs to stay within the terminal. Due to my job my interests currently lie in C programming and computer networks. On top of programming I also really like to write technical blogposts and documentation.\nExperience Working with embedded Linux systems on network switches.","title":"About Me"}]